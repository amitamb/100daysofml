{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amit/anaconda3/envs/tf/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "    def __init__(self, img_rows=28, img_cols=28, channel=1):\n",
    "        \n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        \n",
    "        self.D = None\n",
    "        self.G = None\n",
    "        self.AM = None\n",
    "        self.DM = None\n",
    "    \n",
    "    def discriminator(self):\n",
    "        \n",
    "        if self.D:\n",
    "            return self.D\n",
    "        \n",
    "        self.D = Sequential()\n",
    "        depth = 64\n",
    "        dropout = 0.4\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        self.D.add(Conv2D(depth * 1, 5, strides=2, padding='same', input_shape=input_shape))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "        \n",
    "        self.D.add(Conv2D(depth * 2, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "        \n",
    "        self.D.add(Conv2D(depth * 4, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "        \n",
    "        self.D.add(Conv2D(depth * 4, 5, strides=2, padding='same'))\n",
    "        self.D.add(LeakyReLU(alpha=0.2))\n",
    "        self.D.add(Dropout(dropout))\n",
    "        \n",
    "        self.D.add(Flatten())\n",
    "        self.D.add(Dense(1, activation='sigmoid'))\n",
    "        self.D.summary()\n",
    "        \n",
    "        return self.D\n",
    "    \n",
    "    def generator(self):\n",
    "        \n",
    "        if self.G:\n",
    "            return self.G\n",
    "        \n",
    "        self.G = Sequential()\n",
    "        \n",
    "        depth = 64+64+64+64\n",
    "        dim = 7\n",
    "        \n",
    "        self.G.add(Dense(dim*dim*depth, input_dim=100))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        self.G.add(Reshape((dim, dim, depth)))\n",
    "        \n",
    "        self.G.add(UpSampling2D()) # doubles the row and columns\n",
    "        self.G.add(Conv2DTranspose(int(depth/2), kernel_size=5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        \n",
    "        self.G.add(UpSampling2D()) # doubles the row and columns\n",
    "        self.G.add(Conv2DTranspose(int(depth/4), kernel_size=5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "\n",
    "        self.G.add(Conv2DTranspose(int(depth/8), kernel_size=5, padding='same'))\n",
    "        self.G.add(BatchNormalization(momentum=0.9))\n",
    "        self.G.add(Activation('relu'))\n",
    "        \n",
    "        self.G.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "        self.G.add(Activation('sigmoid'))\n",
    "        self.G.summary()\n",
    "\n",
    "        return self.G\n",
    "    \n",
    "    def discriminator_model(self):\n",
    "        if self.DM:\n",
    "            return self.DM\n",
    "        \n",
    "        optimizer = RMSprop(lr=0.0002, decay=6e-8)\n",
    "        \n",
    "        self.DM = Sequential()\n",
    "        self.DM.add(self.discriminator())\n",
    "        self.DM.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return self.DM\n",
    "    \n",
    "    def adversarial_model(self):\n",
    "        if self.AM:\n",
    "            return self.AM\n",
    "        \n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        \n",
    "        self.AM = Sequential()\n",
    "        self.AM.add(self.generator())\n",
    "        self.AM.add(self.discriminator())\n",
    "        self.AM.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return self.AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_DCGAN(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channel = 1\n",
    "        \n",
    "        self.x_train = input_data.read_data_sets('mnist', one_hot=True).train.images\n",
    "        self.x_train = self.x_train.reshape(-1, self.img_rows, self.img_cols, 1).astype(np.float32)\n",
    "        \n",
    "        self.DCGAN = DCGAN()\n",
    "        self.discriminator = self.DCGAN.discriminator_model()\n",
    "        self.adversarial = self.DCGAN.adversarial_model()\n",
    "        self.generator = self.DCGAN.generator()\n",
    "        \n",
    "    def train(self, train_steps=2000, batch_size=256, save_interval=0):\n",
    "        node_input = None\n",
    "        if save_interval > 0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        \n",
    "        for i in range(train_steps):\n",
    "            images_train = self.x_train[np.random.randint(0, self.x_train.shape[0], size=batch_size), :, :, :]\n",
    "            \n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            \n",
    "            images_fake = self.generator.predict(noise)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([batch_size * 2, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            d_loss = self.discriminator.train_on_batch(x, y)\n",
    "            \n",
    "            y = np.ones([batch_size, 1])\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, 100])\n",
    "            a_loss = self.adversarial.train_on_batch(noise, y)\n",
    "            \n",
    "            log_mesg = \"%d: [D loss: %f, acc: %f]\" % (i, d_loss[0], d_loss[1])\n",
    "            log_mesg = \"%s: [A loss: %f, acc: %f]\" % (log_mesg, a_loss[0], a_loss[1])\n",
    "            \n",
    "            print(log_mesg)\n",
    "            \n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval == 0:\n",
    "                    self.plot_images(save2file=True, samples=noise_input.shape[0],\\\n",
    "                        noise=noise_input, step=(i+1))\n",
    "    \n",
    "    def plot_images(self, save2file=False, fake=True, samples=16, noise=None, step=0):\n",
    "        filename = 'mnist.png'\n",
    "        if fake:\n",
    "            if noise is None:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "            else:\n",
    "                filename = \"mnist_%d.png\" % step\n",
    "            images = self.generator.predict(noise)\n",
    "        else:\n",
    "            i = np.random.randint(0, self.x_train.shape[0], samples)\n",
    "            images = self.x_train[i, :, :, :]\n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(images.shape[0]):\n",
    "            plt.subplot(4, 4, i+1)\n",
    "            image = images[i, :, :, :]\n",
    "            image = np.reshape(image, [self.img_rows, self.img_cols])\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-e36ba267926f>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/amit/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/amit/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/amit/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/amit/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/amit/anaconda3/envs/tf/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        1664      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 2, 256)         1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 2,665,729\n",
      "Trainable params: 2,665,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 12544)             1266944   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 12544)             50176     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 28, 28, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 2,394,241\n",
      "Trainable params: 2,368,705\n",
      "Non-trainable params: 25,536\n",
      "_________________________________________________________________\n",
      "0: [D loss: 0.693491, acc: 0.449219]: [A loss: 1.057604, acc: 0.000000]\n",
      "1: [D loss: 0.669106, acc: 0.591797]: [A loss: 1.023020, acc: 0.000000]\n",
      "2: [D loss: 0.633303, acc: 0.998047]: [A loss: 0.995401, acc: 0.000000]\n",
      "3: [D loss: 0.670777, acc: 0.500000]: [A loss: 1.431017, acc: 0.000000]\n",
      "4: [D loss: 0.610575, acc: 0.832031]: [A loss: 1.061254, acc: 0.000000]\n",
      "5: [D loss: 0.551237, acc: 0.992188]: [A loss: 1.084678, acc: 0.000000]\n",
      "6: [D loss: 0.531803, acc: 0.703125]: [A loss: 1.394112, acc: 0.000000]\n",
      "7: [D loss: 0.470685, acc: 0.978516]: [A loss: 1.238732, acc: 0.000000]\n",
      "8: [D loss: 0.492867, acc: 0.564453]: [A loss: 1.962772, acc: 0.000000]\n",
      "9: [D loss: 0.467573, acc: 0.986328]: [A loss: 1.004130, acc: 0.000000]\n",
      "10: [D loss: 0.526491, acc: 0.505859]: [A loss: 1.810261, acc: 0.000000]\n",
      "11: [D loss: 0.374642, acc: 1.000000]: [A loss: 1.223488, acc: 0.000000]\n",
      "12: [D loss: 0.384532, acc: 0.802734]: [A loss: 1.773050, acc: 0.000000]\n",
      "13: [D loss: 0.292131, acc: 1.000000]: [A loss: 1.518140, acc: 0.000000]\n",
      "14: [D loss: 0.362311, acc: 0.800781]: [A loss: 2.284173, acc: 0.000000]\n",
      "15: [D loss: 0.271874, acc: 1.000000]: [A loss: 1.489903, acc: 0.000000]\n",
      "16: [D loss: 0.412104, acc: 0.673828]: [A loss: 2.476637, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17: [D loss: 0.278494, acc: 1.000000]: [A loss: 1.564559, acc: 0.000000]\n",
      "18: [D loss: 0.366605, acc: 0.791016]: [A loss: 2.359169, acc: 0.000000]\n",
      "19: [D loss: 0.244212, acc: 1.000000]: [A loss: 1.648950, acc: 0.000000]\n",
      "20: [D loss: 0.364838, acc: 0.777344]: [A loss: 2.549556, acc: 0.000000]\n",
      "21: [D loss: 0.240062, acc: 0.992188]: [A loss: 1.507174, acc: 0.000000]\n",
      "22: [D loss: 0.414811, acc: 0.652344]: [A loss: 2.679068, acc: 0.000000]\n",
      "23: [D loss: 0.232508, acc: 0.990234]: [A loss: 1.479911, acc: 0.000000]\n",
      "24: [D loss: 0.341961, acc: 0.810547]: [A loss: 2.482317, acc: 0.000000]\n",
      "25: [D loss: 0.191316, acc: 0.998047]: [A loss: 1.780959, acc: 0.000000]\n",
      "26: [D loss: 0.303024, acc: 0.878906]: [A loss: 2.707196, acc: 0.000000]\n",
      "27: [D loss: 0.179530, acc: 0.990234]: [A loss: 1.620830, acc: 0.000000]\n",
      "28: [D loss: 0.334982, acc: 0.794922]: [A loss: 3.054865, acc: 0.000000]\n",
      "29: [D loss: 0.200308, acc: 0.970703]: [A loss: 1.429214, acc: 0.011719]\n",
      "30: [D loss: 0.322060, acc: 0.826172]: [A loss: 2.692429, acc: 0.000000]\n",
      "31: [D loss: 0.147180, acc: 0.994141]: [A loss: 1.554555, acc: 0.000000]\n",
      "32: [D loss: 0.228142, acc: 0.933594]: [A loss: 2.506627, acc: 0.000000]\n",
      "33: [D loss: 0.141838, acc: 0.996094]: [A loss: 1.893067, acc: 0.000000]\n",
      "34: [D loss: 0.154397, acc: 0.988281]: [A loss: 2.133911, acc: 0.000000]\n",
      "35: [D loss: 0.162277, acc: 0.990234]: [A loss: 2.393933, acc: 0.000000]\n",
      "36: [D loss: 0.106853, acc: 0.984375]: [A loss: 1.245164, acc: 0.070312]\n",
      "37: [D loss: 0.232820, acc: 0.906250]: [A loss: 3.352584, acc: 0.000000]\n",
      "38: [D loss: 0.231758, acc: 0.929688]: [A loss: 0.589154, acc: 0.687500]\n",
      "39: [D loss: 0.386506, acc: 0.763672]: [A loss: 2.947141, acc: 0.000000]\n",
      "40: [D loss: 0.193145, acc: 0.945312]: [A loss: 0.687478, acc: 0.554688]\n",
      "41: [D loss: 0.245271, acc: 0.900391]: [A loss: 1.767742, acc: 0.000000]\n",
      "42: [D loss: 0.104860, acc: 0.994141]: [A loss: 1.132718, acc: 0.132812]\n",
      "43: [D loss: 0.147916, acc: 0.974609]: [A loss: 1.648064, acc: 0.003906]\n",
      "44: [D loss: 0.101455, acc: 0.988281]: [A loss: 1.053481, acc: 0.179688]\n",
      "45: [D loss: 0.122058, acc: 0.978516]: [A loss: 1.199403, acc: 0.117188]\n",
      "46: [D loss: 0.131424, acc: 0.988281]: [A loss: 1.102267, acc: 0.210938]\n",
      "47: [D loss: 0.074836, acc: 0.992188]: [A loss: 0.492716, acc: 0.792969]\n",
      "48: [D loss: 0.100985, acc: 0.986328]: [A loss: 1.205482, acc: 0.105469]\n",
      "49: [D loss: 0.069861, acc: 0.996094]: [A loss: 0.672065, acc: 0.578125]\n",
      "50: [D loss: 0.169332, acc: 0.957031]: [A loss: 2.046399, acc: 0.000000]\n",
      "51: [D loss: 0.136978, acc: 0.976562]: [A loss: 0.315835, acc: 0.945312]\n",
      "52: [D loss: 0.277344, acc: 0.878906]: [A loss: 3.056572, acc: 0.000000]\n",
      "53: [D loss: 0.484528, acc: 0.853516]: [A loss: 0.067105, acc: 1.000000]\n",
      "54: [D loss: 1.113538, acc: 0.501953]: [A loss: 2.215739, acc: 0.000000]\n",
      "55: [D loss: 0.305122, acc: 0.906250]: [A loss: 0.438617, acc: 0.906250]\n",
      "56: [D loss: 0.305638, acc: 0.853516]: [A loss: 1.162982, acc: 0.082031]\n",
      "57: [D loss: 0.213046, acc: 0.955078]: [A loss: 0.960367, acc: 0.195312]\n",
      "58: [D loss: 0.240676, acc: 0.927734]: [A loss: 1.227013, acc: 0.046875]\n",
      "59: [D loss: 0.291738, acc: 0.886719]: [A loss: 1.502049, acc: 0.011719]\n",
      "60: [D loss: 0.241805, acc: 0.947266]: [A loss: 0.882381, acc: 0.320312]\n",
      "61: [D loss: 0.244353, acc: 0.929688]: [A loss: 1.198328, acc: 0.054688]\n",
      "62: [D loss: 0.362573, acc: 0.816406]: [A loss: 1.816294, acc: 0.000000]\n",
      "63: [D loss: 0.311886, acc: 0.925781]: [A loss: 0.492843, acc: 0.808594]\n",
      "64: [D loss: 0.536084, acc: 0.605469]: [A loss: 2.616466, acc: 0.000000]\n",
      "65: [D loss: 0.633649, acc: 0.796875]: [A loss: 0.272757, acc: 0.984375]\n",
      "66: [D loss: 0.821005, acc: 0.507812]: [A loss: 1.724908, acc: 0.000000]\n",
      "67: [D loss: 0.281447, acc: 0.953125]: [A loss: 0.657651, acc: 0.621094]\n",
      "68: [D loss: 0.386177, acc: 0.736328]: [A loss: 1.494803, acc: 0.000000]\n",
      "69: [D loss: 0.300941, acc: 0.921875]: [A loss: 0.891075, acc: 0.242188]\n",
      "70: [D loss: 0.362252, acc: 0.794922]: [A loss: 1.595546, acc: 0.000000]\n",
      "71: [D loss: 0.340999, acc: 0.894531]: [A loss: 0.971236, acc: 0.171875]\n",
      "72: [D loss: 0.411829, acc: 0.748047]: [A loss: 1.648653, acc: 0.000000]\n",
      "73: [D loss: 0.338351, acc: 0.902344]: [A loss: 0.966938, acc: 0.191406]\n",
      "74: [D loss: 0.412108, acc: 0.738281]: [A loss: 1.805527, acc: 0.000000]\n",
      "75: [D loss: 0.353870, acc: 0.900391]: [A loss: 0.744492, acc: 0.453125]\n",
      "76: [D loss: 0.521170, acc: 0.607422]: [A loss: 2.412490, acc: 0.000000]\n",
      "77: [D loss: 0.513322, acc: 0.818359]: [A loss: 0.269720, acc: 0.992188]\n",
      "78: [D loss: 0.966878, acc: 0.500000]: [A loss: 1.776207, acc: 0.000000]\n",
      "79: [D loss: 0.383842, acc: 0.912109]: [A loss: 0.623589, acc: 0.671875]\n",
      "80: [D loss: 0.582487, acc: 0.558594]: [A loss: 1.679610, acc: 0.000000]\n",
      "81: [D loss: 0.371644, acc: 0.929688]: [A loss: 0.764900, acc: 0.386719]\n",
      "82: [D loss: 0.531314, acc: 0.580078]: [A loss: 1.761299, acc: 0.000000]\n",
      "83: [D loss: 0.370418, acc: 0.925781]: [A loss: 0.735089, acc: 0.425781]\n",
      "84: [D loss: 0.574556, acc: 0.552734]: [A loss: 1.923826, acc: 0.000000]\n",
      "85: [D loss: 0.390583, acc: 0.910156]: [A loss: 0.562479, acc: 0.765625]\n",
      "86: [D loss: 0.681607, acc: 0.513672]: [A loss: 1.878114, acc: 0.000000]\n",
      "87: [D loss: 0.432614, acc: 0.898438]: [A loss: 0.523236, acc: 0.835938]\n",
      "88: [D loss: 0.717194, acc: 0.505859]: [A loss: 1.728667, acc: 0.000000]\n",
      "89: [D loss: 0.398667, acc: 0.908203]: [A loss: 0.652254, acc: 0.628906]\n",
      "90: [D loss: 0.609568, acc: 0.541016]: [A loss: 1.724617, acc: 0.000000]\n",
      "91: [D loss: 0.402962, acc: 0.912109]: [A loss: 0.733082, acc: 0.437500]\n",
      "92: [D loss: 0.594221, acc: 0.544922]: [A loss: 1.791921, acc: 0.000000]\n",
      "93: [D loss: 0.422598, acc: 0.900391]: [A loss: 0.663551, acc: 0.609375]\n",
      "94: [D loss: 0.580568, acc: 0.546875]: [A loss: 1.752245, acc: 0.000000]\n",
      "95: [D loss: 0.409080, acc: 0.900391]: [A loss: 0.673466, acc: 0.566406]\n",
      "96: [D loss: 0.603794, acc: 0.539062]: [A loss: 1.840747, acc: 0.000000]\n",
      "97: [D loss: 0.413125, acc: 0.902344]: [A loss: 0.593091, acc: 0.707031]\n",
      "98: [D loss: 0.651188, acc: 0.515625]: [A loss: 1.863327, acc: 0.000000]\n",
      "99: [D loss: 0.421021, acc: 0.908203]: [A loss: 0.632312, acc: 0.671875]\n",
      "100: [D loss: 0.660057, acc: 0.515625]: [A loss: 1.806353, acc: 0.000000]\n",
      "101: [D loss: 0.413431, acc: 0.914062]: [A loss: 0.665455, acc: 0.597656]\n",
      "102: [D loss: 0.646326, acc: 0.515625]: [A loss: 1.839364, acc: 0.000000]\n",
      "103: [D loss: 0.420163, acc: 0.894531]: [A loss: 0.629282, acc: 0.652344]\n",
      "104: [D loss: 0.645197, acc: 0.517578]: [A loss: 1.834039, acc: 0.000000]\n",
      "105: [D loss: 0.408715, acc: 0.919922]: [A loss: 0.694706, acc: 0.531250]\n",
      "106: [D loss: 0.638184, acc: 0.525391]: [A loss: 1.882954, acc: 0.000000]\n",
      "107: [D loss: 0.413689, acc: 0.908203]: [A loss: 0.681896, acc: 0.570312]\n",
      "108: [D loss: 0.647253, acc: 0.529297]: [A loss: 1.950430, acc: 0.000000]\n",
      "109: [D loss: 0.442702, acc: 0.892578]: [A loss: 0.586061, acc: 0.742188]\n",
      "110: [D loss: 0.700792, acc: 0.511719]: [A loss: 1.847009, acc: 0.000000]\n",
      "111: [D loss: 0.415682, acc: 0.931641]: [A loss: 0.740905, acc: 0.429688]\n",
      "112: [D loss: 0.607144, acc: 0.539062]: [A loss: 1.753303, acc: 0.000000]\n",
      "113: [D loss: 0.454919, acc: 0.890625]: [A loss: 0.714803, acc: 0.488281]\n",
      "114: [D loss: 0.635249, acc: 0.523438]: [A loss: 1.963837, acc: 0.000000]\n",
      "115: [D loss: 0.424992, acc: 0.910156]: [A loss: 0.711172, acc: 0.519531]\n",
      "116: [D loss: 0.672480, acc: 0.523438]: [A loss: 1.972587, acc: 0.000000]\n",
      "117: [D loss: 0.465122, acc: 0.875000]: [A loss: 0.654808, acc: 0.585938]\n",
      "118: [D loss: 0.707357, acc: 0.515625]: [A loss: 1.886402, acc: 0.000000]\n",
      "119: [D loss: 0.454194, acc: 0.884766]: [A loss: 0.713016, acc: 0.480469]\n",
      "120: [D loss: 0.682301, acc: 0.507812]: [A loss: 1.849990, acc: 0.000000]\n",
      "121: [D loss: 0.459226, acc: 0.888672]: [A loss: 0.681631, acc: 0.535156]\n",
      "122: [D loss: 0.696679, acc: 0.515625]: [A loss: 1.896248, acc: 0.000000]\n",
      "123: [D loss: 0.467215, acc: 0.892578]: [A loss: 0.737723, acc: 0.429688]\n",
      "124: [D loss: 0.690634, acc: 0.513672]: [A loss: 1.947896, acc: 0.000000]\n",
      "125: [D loss: 0.480187, acc: 0.851562]: [A loss: 0.644371, acc: 0.652344]\n",
      "126: [D loss: 0.712172, acc: 0.511719]: [A loss: 1.874994, acc: 0.000000]\n",
      "127: [D loss: 0.484150, acc: 0.867188]: [A loss: 0.695098, acc: 0.511719]\n",
      "128: [D loss: 0.708577, acc: 0.509766]: [A loss: 1.893553, acc: 0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129: [D loss: 0.487417, acc: 0.857422]: [A loss: 0.691451, acc: 0.531250]\n",
      "130: [D loss: 0.689037, acc: 0.517578]: [A loss: 1.803198, acc: 0.000000]\n",
      "131: [D loss: 0.502905, acc: 0.828125]: [A loss: 0.749674, acc: 0.414062]\n",
      "132: [D loss: 0.686052, acc: 0.509766]: [A loss: 1.937149, acc: 0.000000]\n",
      "133: [D loss: 0.508239, acc: 0.826172]: [A loss: 0.709964, acc: 0.511719]\n",
      "134: [D loss: 0.723173, acc: 0.505859]: [A loss: 2.015545, acc: 0.000000]\n",
      "135: [D loss: 0.525937, acc: 0.820312]: [A loss: 0.630917, acc: 0.652344]\n",
      "136: [D loss: 0.751542, acc: 0.509766]: [A loss: 1.902254, acc: 0.000000]\n",
      "137: [D loss: 0.507860, acc: 0.812500]: [A loss: 0.638098, acc: 0.660156]\n",
      "138: [D loss: 0.760279, acc: 0.501953]: [A loss: 1.912835, acc: 0.000000]\n",
      "139: [D loss: 0.515679, acc: 0.835938]: [A loss: 0.692300, acc: 0.511719]\n",
      "140: [D loss: 0.765181, acc: 0.509766]: [A loss: 1.875261, acc: 0.000000]\n",
      "141: [D loss: 0.515278, acc: 0.839844]: [A loss: 0.660971, acc: 0.597656]\n",
      "142: [D loss: 0.747135, acc: 0.501953]: [A loss: 1.785236, acc: 0.000000]\n",
      "143: [D loss: 0.517602, acc: 0.845703]: [A loss: 0.778065, acc: 0.375000]\n",
      "144: [D loss: 0.696611, acc: 0.503906]: [A loss: 1.906636, acc: 0.000000]\n",
      "145: [D loss: 0.516514, acc: 0.847656]: [A loss: 0.671651, acc: 0.617188]\n",
      "146: [D loss: 0.712384, acc: 0.507812]: [A loss: 1.948489, acc: 0.000000]\n",
      "147: [D loss: 0.542651, acc: 0.759766]: [A loss: 0.610607, acc: 0.679688]\n",
      "148: [D loss: 0.758557, acc: 0.505859]: [A loss: 1.839495, acc: 0.000000]\n",
      "149: [D loss: 0.514291, acc: 0.845703]: [A loss: 0.712529, acc: 0.472656]\n",
      "150: [D loss: 0.713418, acc: 0.511719]: [A loss: 1.863883, acc: 0.000000]\n",
      "151: [D loss: 0.513425, acc: 0.839844]: [A loss: 0.696052, acc: 0.531250]\n",
      "152: [D loss: 0.733707, acc: 0.519531]: [A loss: 1.982361, acc: 0.000000]\n",
      "153: [D loss: 0.530122, acc: 0.806641]: [A loss: 0.631339, acc: 0.640625]\n",
      "154: [D loss: 0.751167, acc: 0.503906]: [A loss: 1.901544, acc: 0.000000]\n",
      "155: [D loss: 0.536052, acc: 0.791016]: [A loss: 0.637157, acc: 0.640625]\n",
      "156: [D loss: 0.756423, acc: 0.500000]: [A loss: 1.860916, acc: 0.000000]\n",
      "157: [D loss: 0.560023, acc: 0.771484]: [A loss: 0.736635, acc: 0.449219]\n",
      "158: [D loss: 0.728139, acc: 0.503906]: [A loss: 1.868159, acc: 0.000000]\n",
      "159: [D loss: 0.533498, acc: 0.828125]: [A loss: 0.666116, acc: 0.578125]\n",
      "160: [D loss: 0.756316, acc: 0.503906]: [A loss: 1.979619, acc: 0.000000]\n",
      "161: [D loss: 0.566148, acc: 0.746094]: [A loss: 0.616431, acc: 0.671875]\n",
      "162: [D loss: 0.755927, acc: 0.500000]: [A loss: 1.700345, acc: 0.000000]\n",
      "163: [D loss: 0.541205, acc: 0.814453]: [A loss: 0.785462, acc: 0.332031]\n",
      "164: [D loss: 0.707675, acc: 0.507812]: [A loss: 1.754375, acc: 0.000000]\n",
      "165: [D loss: 0.541294, acc: 0.802734]: [A loss: 0.726255, acc: 0.468750]\n",
      "166: [D loss: 0.715129, acc: 0.509766]: [A loss: 1.813430, acc: 0.000000]\n",
      "167: [D loss: 0.547963, acc: 0.791016]: [A loss: 0.721214, acc: 0.464844]\n",
      "168: [D loss: 0.734247, acc: 0.503906]: [A loss: 1.920359, acc: 0.000000]\n",
      "169: [D loss: 0.564594, acc: 0.777344]: [A loss: 0.602906, acc: 0.710938]\n",
      "170: [D loss: 0.775688, acc: 0.501953]: [A loss: 1.703543, acc: 0.000000]\n",
      "171: [D loss: 0.569039, acc: 0.755859]: [A loss: 0.664935, acc: 0.593750]\n",
      "172: [D loss: 0.706965, acc: 0.505859]: [A loss: 1.594349, acc: 0.000000]\n",
      "173: [D loss: 0.545027, acc: 0.828125]: [A loss: 0.819533, acc: 0.304688]\n",
      "174: [D loss: 0.677785, acc: 0.519531]: [A loss: 1.669083, acc: 0.000000]\n",
      "175: [D loss: 0.553596, acc: 0.769531]: [A loss: 0.672915, acc: 0.570312]\n",
      "176: [D loss: 0.742913, acc: 0.503906]: [A loss: 1.751639, acc: 0.000000]\n",
      "177: [D loss: 0.581876, acc: 0.736328]: [A loss: 0.617936, acc: 0.656250]\n",
      "178: [D loss: 0.752026, acc: 0.500000]: [A loss: 1.662549, acc: 0.000000]\n",
      "179: [D loss: 0.588071, acc: 0.730469]: [A loss: 0.632744, acc: 0.664062]\n",
      "180: [D loss: 0.715724, acc: 0.503906]: [A loss: 1.564531, acc: 0.000000]\n",
      "181: [D loss: 0.569213, acc: 0.765625]: [A loss: 0.668046, acc: 0.605469]\n",
      "182: [D loss: 0.712197, acc: 0.525391]: [A loss: 1.571674, acc: 0.000000]\n",
      "183: [D loss: 0.579350, acc: 0.763672]: [A loss: 0.662284, acc: 0.585938]\n",
      "184: [D loss: 0.736605, acc: 0.503906]: [A loss: 1.585019, acc: 0.000000]\n",
      "185: [D loss: 0.567620, acc: 0.794922]: [A loss: 0.664756, acc: 0.578125]\n",
      "186: [D loss: 0.712178, acc: 0.513672]: [A loss: 1.550186, acc: 0.000000]\n",
      "187: [D loss: 0.569148, acc: 0.761719]: [A loss: 0.638610, acc: 0.644531]\n",
      "188: [D loss: 0.717284, acc: 0.509766]: [A loss: 1.606102, acc: 0.000000]\n",
      "189: [D loss: 0.568793, acc: 0.759766]: [A loss: 0.663550, acc: 0.574219]\n",
      "190: [D loss: 0.726094, acc: 0.505859]: [A loss: 1.628455, acc: 0.000000]\n",
      "191: [D loss: 0.566252, acc: 0.769531]: [A loss: 0.634711, acc: 0.644531]\n",
      "192: [D loss: 0.729644, acc: 0.513672]: [A loss: 1.615148, acc: 0.000000]\n",
      "193: [D loss: 0.576656, acc: 0.740234]: [A loss: 0.666869, acc: 0.605469]\n",
      "194: [D loss: 0.730179, acc: 0.507812]: [A loss: 1.595753, acc: 0.000000]\n",
      "195: [D loss: 0.579348, acc: 0.761719]: [A loss: 0.712150, acc: 0.492188]\n",
      "196: [D loss: 0.714453, acc: 0.501953]: [A loss: 1.576508, acc: 0.000000]\n",
      "197: [D loss: 0.589217, acc: 0.742188]: [A loss: 0.675682, acc: 0.578125]\n",
      "198: [D loss: 0.706289, acc: 0.509766]: [A loss: 1.605489, acc: 0.000000]\n",
      "199: [D loss: 0.589638, acc: 0.748047]: [A loss: 0.718908, acc: 0.472656]\n",
      "200: [D loss: 0.733602, acc: 0.511719]: [A loss: 1.654505, acc: 0.000000]\n",
      "201: [D loss: 0.604323, acc: 0.726562]: [A loss: 0.671494, acc: 0.562500]\n",
      "202: [D loss: 0.726254, acc: 0.505859]: [A loss: 1.482386, acc: 0.000000]\n",
      "203: [D loss: 0.602638, acc: 0.718750]: [A loss: 0.809097, acc: 0.285156]\n",
      "204: [D loss: 0.694308, acc: 0.523438]: [A loss: 1.487000, acc: 0.000000]\n",
      "205: [D loss: 0.614997, acc: 0.722656]: [A loss: 0.787091, acc: 0.316406]\n",
      "206: [D loss: 0.714703, acc: 0.519531]: [A loss: 1.532242, acc: 0.000000]\n",
      "207: [D loss: 0.622710, acc: 0.683594]: [A loss: 0.745853, acc: 0.425781]\n",
      "208: [D loss: 0.705794, acc: 0.519531]: [A loss: 1.540365, acc: 0.000000]\n",
      "209: [D loss: 0.610804, acc: 0.705078]: [A loss: 0.772724, acc: 0.382812]\n",
      "210: [D loss: 0.710237, acc: 0.509766]: [A loss: 1.505151, acc: 0.003906]\n",
      "211: [D loss: 0.622707, acc: 0.691406]: [A loss: 0.692285, acc: 0.535156]\n",
      "212: [D loss: 0.724507, acc: 0.523438]: [A loss: 1.571593, acc: 0.000000]\n",
      "213: [D loss: 0.639962, acc: 0.650391]: [A loss: 0.679271, acc: 0.546875]\n",
      "214: [D loss: 0.756719, acc: 0.517578]: [A loss: 1.467512, acc: 0.000000]\n",
      "215: [D loss: 0.651010, acc: 0.605469]: [A loss: 0.686076, acc: 0.523438]\n",
      "216: [D loss: 0.746960, acc: 0.511719]: [A loss: 1.392449, acc: 0.000000]\n",
      "217: [D loss: 0.630074, acc: 0.664062]: [A loss: 0.808908, acc: 0.324219]\n",
      "218: [D loss: 0.690077, acc: 0.539062]: [A loss: 1.283391, acc: 0.003906]\n",
      "219: [D loss: 0.630077, acc: 0.671875]: [A loss: 0.756811, acc: 0.386719]\n",
      "220: [D loss: 0.719237, acc: 0.501953]: [A loss: 1.453423, acc: 0.000000]\n",
      "221: [D loss: 0.640427, acc: 0.652344]: [A loss: 0.589705, acc: 0.730469]\n",
      "222: [D loss: 0.758716, acc: 0.500000]: [A loss: 1.473529, acc: 0.000000]\n",
      "223: [D loss: 0.638523, acc: 0.621094]: [A loss: 0.557191, acc: 0.792969]\n",
      "224: [D loss: 0.750320, acc: 0.501953]: [A loss: 1.291852, acc: 0.000000]\n",
      "225: [D loss: 0.646843, acc: 0.660156]: [A loss: 0.660845, acc: 0.632812]\n",
      "226: [D loss: 0.733090, acc: 0.507812]: [A loss: 1.300154, acc: 0.003906]\n",
      "227: [D loss: 0.641567, acc: 0.664062]: [A loss: 0.761872, acc: 0.359375]\n",
      "228: [D loss: 0.706012, acc: 0.513672]: [A loss: 1.380546, acc: 0.003906]\n",
      "229: [D loss: 0.623337, acc: 0.699219]: [A loss: 0.672781, acc: 0.601562]\n",
      "230: [D loss: 0.725510, acc: 0.511719]: [A loss: 1.363963, acc: 0.000000]\n",
      "231: [D loss: 0.645761, acc: 0.646484]: [A loss: 0.664331, acc: 0.574219]\n",
      "232: [D loss: 0.738372, acc: 0.505859]: [A loss: 1.407607, acc: 0.000000]\n",
      "233: [D loss: 0.641602, acc: 0.646484]: [A loss: 0.590744, acc: 0.777344]\n",
      "234: [D loss: 0.750978, acc: 0.501953]: [A loss: 1.320188, acc: 0.007812]\n",
      "235: [D loss: 0.634336, acc: 0.656250]: [A loss: 0.658924, acc: 0.617188]\n",
      "236: [D loss: 0.723257, acc: 0.507812]: [A loss: 1.322053, acc: 0.000000]\n",
      "237: [D loss: 0.637114, acc: 0.673828]: [A loss: 0.686427, acc: 0.503906]\n",
      "238: [D loss: 0.700488, acc: 0.501953]: [A loss: 1.269741, acc: 0.000000]\n",
      "239: [D loss: 0.635965, acc: 0.669922]: [A loss: 0.678021, acc: 0.527344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240: [D loss: 0.736712, acc: 0.505859]: [A loss: 1.355250, acc: 0.000000]\n",
      "241: [D loss: 0.632820, acc: 0.652344]: [A loss: 0.617990, acc: 0.699219]\n",
      "242: [D loss: 0.749679, acc: 0.500000]: [A loss: 1.326206, acc: 0.000000]\n",
      "243: [D loss: 0.636089, acc: 0.652344]: [A loss: 0.672411, acc: 0.570312]\n",
      "244: [D loss: 0.732233, acc: 0.511719]: [A loss: 1.274390, acc: 0.003906]\n",
      "245: [D loss: 0.638003, acc: 0.683594]: [A loss: 0.629669, acc: 0.679688]\n",
      "246: [D loss: 0.724830, acc: 0.511719]: [A loss: 1.240898, acc: 0.000000]\n",
      "247: [D loss: 0.656540, acc: 0.638672]: [A loss: 0.676595, acc: 0.535156]\n",
      "248: [D loss: 0.727973, acc: 0.509766]: [A loss: 1.214265, acc: 0.003906]\n",
      "249: [D loss: 0.644057, acc: 0.669922]: [A loss: 0.696581, acc: 0.507812]\n",
      "250: [D loss: 0.714858, acc: 0.513672]: [A loss: 1.203082, acc: 0.003906]\n",
      "251: [D loss: 0.642258, acc: 0.638672]: [A loss: 0.742810, acc: 0.382812]\n",
      "252: [D loss: 0.695647, acc: 0.511719]: [A loss: 1.238474, acc: 0.000000]\n",
      "253: [D loss: 0.653257, acc: 0.658203]: [A loss: 0.696416, acc: 0.539062]\n",
      "254: [D loss: 0.724858, acc: 0.507812]: [A loss: 1.311759, acc: 0.000000]\n",
      "255: [D loss: 0.664420, acc: 0.589844]: [A loss: 0.614916, acc: 0.671875]\n",
      "256: [D loss: 0.734557, acc: 0.511719]: [A loss: 1.328035, acc: 0.000000]\n",
      "257: [D loss: 0.639210, acc: 0.654297]: [A loss: 0.600678, acc: 0.746094]\n",
      "258: [D loss: 0.741179, acc: 0.503906]: [A loss: 1.197959, acc: 0.000000]\n",
      "259: [D loss: 0.651852, acc: 0.636719]: [A loss: 0.689356, acc: 0.507812]\n",
      "260: [D loss: 0.699737, acc: 0.521484]: [A loss: 1.070130, acc: 0.007812]\n",
      "261: [D loss: 0.649067, acc: 0.644531]: [A loss: 0.823413, acc: 0.214844]\n",
      "262: [D loss: 0.686904, acc: 0.521484]: [A loss: 1.121834, acc: 0.007812]\n",
      "263: [D loss: 0.660453, acc: 0.611328]: [A loss: 0.755745, acc: 0.375000]\n",
      "264: [D loss: 0.695711, acc: 0.515625]: [A loss: 1.222092, acc: 0.000000]\n",
      "265: [D loss: 0.666100, acc: 0.599609]: [A loss: 0.690992, acc: 0.550781]\n",
      "266: [D loss: 0.735938, acc: 0.511719]: [A loss: 1.361911, acc: 0.000000]\n",
      "267: [D loss: 0.672444, acc: 0.572266]: [A loss: 0.562759, acc: 0.847656]\n",
      "268: [D loss: 0.744384, acc: 0.501953]: [A loss: 1.206066, acc: 0.007812]\n",
      "269: [D loss: 0.671348, acc: 0.583984]: [A loss: 0.644110, acc: 0.652344]\n",
      "270: [D loss: 0.730118, acc: 0.503906]: [A loss: 1.145689, acc: 0.000000]\n",
      "271: [D loss: 0.655544, acc: 0.623047]: [A loss: 0.675232, acc: 0.558594]\n",
      "272: [D loss: 0.714686, acc: 0.500000]: [A loss: 1.087414, acc: 0.007812]\n",
      "273: [D loss: 0.658322, acc: 0.650391]: [A loss: 0.690831, acc: 0.503906]\n",
      "274: [D loss: 0.690157, acc: 0.529297]: [A loss: 1.077319, acc: 0.003906]\n",
      "275: [D loss: 0.659286, acc: 0.613281]: [A loss: 0.755933, acc: 0.363281]\n",
      "276: [D loss: 0.699255, acc: 0.513672]: [A loss: 1.153056, acc: 0.000000]\n",
      "277: [D loss: 0.660622, acc: 0.625000]: [A loss: 0.677407, acc: 0.562500]\n",
      "278: [D loss: 0.732936, acc: 0.505859]: [A loss: 1.255362, acc: 0.000000]\n",
      "279: [D loss: 0.664684, acc: 0.591797]: [A loss: 0.631436, acc: 0.687500]\n",
      "280: [D loss: 0.728255, acc: 0.509766]: [A loss: 1.187095, acc: 0.000000]\n",
      "281: [D loss: 0.672991, acc: 0.593750]: [A loss: 0.628853, acc: 0.738281]\n",
      "282: [D loss: 0.731940, acc: 0.501953]: [A loss: 1.110238, acc: 0.007812]\n",
      "283: [D loss: 0.665982, acc: 0.613281]: [A loss: 0.670848, acc: 0.582031]\n",
      "284: [D loss: 0.713391, acc: 0.513672]: [A loss: 1.065793, acc: 0.003906]\n",
      "285: [D loss: 0.654866, acc: 0.658203]: [A loss: 0.674775, acc: 0.593750]\n",
      "286: [D loss: 0.711971, acc: 0.503906]: [A loss: 1.064611, acc: 0.000000]\n",
      "287: [D loss: 0.669906, acc: 0.630859]: [A loss: 0.704976, acc: 0.507812]\n",
      "288: [D loss: 0.701403, acc: 0.525391]: [A loss: 1.105677, acc: 0.003906]\n",
      "289: [D loss: 0.654228, acc: 0.669922]: [A loss: 0.683655, acc: 0.558594]\n",
      "290: [D loss: 0.722482, acc: 0.507812]: [A loss: 1.134762, acc: 0.003906]\n",
      "291: [D loss: 0.665839, acc: 0.615234]: [A loss: 0.689073, acc: 0.519531]\n",
      "292: [D loss: 0.716675, acc: 0.507812]: [A loss: 1.167496, acc: 0.003906]\n",
      "293: [D loss: 0.672513, acc: 0.570312]: [A loss: 0.660640, acc: 0.597656]\n",
      "294: [D loss: 0.721475, acc: 0.509766]: [A loss: 1.113441, acc: 0.000000]\n",
      "295: [D loss: 0.667861, acc: 0.583984]: [A loss: 0.698234, acc: 0.527344]\n",
      "296: [D loss: 0.708038, acc: 0.498047]: [A loss: 1.036927, acc: 0.019531]\n",
      "297: [D loss: 0.673201, acc: 0.587891]: [A loss: 0.757572, acc: 0.320312]\n",
      "298: [D loss: 0.684964, acc: 0.523438]: [A loss: 0.987639, acc: 0.035156]\n",
      "299: [D loss: 0.671901, acc: 0.580078]: [A loss: 0.764174, acc: 0.335938]\n",
      "300: [D loss: 0.695470, acc: 0.529297]: [A loss: 1.098023, acc: 0.015625]\n",
      "301: [D loss: 0.688752, acc: 0.537109]: [A loss: 0.711503, acc: 0.460938]\n",
      "302: [D loss: 0.706957, acc: 0.531250]: [A loss: 1.154171, acc: 0.000000]\n",
      "303: [D loss: 0.670300, acc: 0.603516]: [A loss: 0.665655, acc: 0.617188]\n",
      "304: [D loss: 0.724643, acc: 0.523438]: [A loss: 1.125361, acc: 0.000000]\n",
      "305: [D loss: 0.677732, acc: 0.576172]: [A loss: 0.692155, acc: 0.492188]\n",
      "306: [D loss: 0.706434, acc: 0.517578]: [A loss: 1.071214, acc: 0.000000]\n",
      "307: [D loss: 0.674795, acc: 0.593750]: [A loss: 0.672056, acc: 0.578125]\n",
      "308: [D loss: 0.710798, acc: 0.523438]: [A loss: 1.048378, acc: 0.007812]\n",
      "309: [D loss: 0.673231, acc: 0.601562]: [A loss: 0.674875, acc: 0.542969]\n",
      "310: [D loss: 0.709620, acc: 0.517578]: [A loss: 1.100380, acc: 0.000000]\n",
      "311: [D loss: 0.680656, acc: 0.552734]: [A loss: 0.645275, acc: 0.644531]\n",
      "312: [D loss: 0.708681, acc: 0.501953]: [A loss: 0.992835, acc: 0.007812]\n",
      "313: [D loss: 0.677935, acc: 0.617188]: [A loss: 0.692476, acc: 0.515625]\n",
      "314: [D loss: 0.701810, acc: 0.511719]: [A loss: 0.969399, acc: 0.015625]\n",
      "315: [D loss: 0.679856, acc: 0.566406]: [A loss: 0.750100, acc: 0.300781]\n",
      "316: [D loss: 0.698863, acc: 0.531250]: [A loss: 1.011186, acc: 0.019531]\n",
      "317: [D loss: 0.676188, acc: 0.564453]: [A loss: 0.773462, acc: 0.273438]\n",
      "318: [D loss: 0.694041, acc: 0.529297]: [A loss: 0.963939, acc: 0.023438]\n",
      "319: [D loss: 0.679296, acc: 0.570312]: [A loss: 0.764183, acc: 0.308594]\n",
      "320: [D loss: 0.694337, acc: 0.525391]: [A loss: 1.026467, acc: 0.000000]\n",
      "321: [D loss: 0.672106, acc: 0.609375]: [A loss: 0.701198, acc: 0.488281]\n",
      "322: [D loss: 0.714317, acc: 0.507812]: [A loss: 1.123488, acc: 0.000000]\n",
      "323: [D loss: 0.677537, acc: 0.533203]: [A loss: 0.612311, acc: 0.765625]\n",
      "324: [D loss: 0.724111, acc: 0.498047]: [A loss: 1.095083, acc: 0.000000]\n",
      "325: [D loss: 0.689167, acc: 0.515625]: [A loss: 0.621639, acc: 0.753906]\n",
      "326: [D loss: 0.725117, acc: 0.501953]: [A loss: 0.999278, acc: 0.003906]\n",
      "327: [D loss: 0.686710, acc: 0.548828]: [A loss: 0.657732, acc: 0.632812]\n",
      "328: [D loss: 0.706041, acc: 0.515625]: [A loss: 0.906143, acc: 0.031250]\n",
      "329: [D loss: 0.672853, acc: 0.591797]: [A loss: 0.729221, acc: 0.367188]\n",
      "330: [D loss: 0.690557, acc: 0.521484]: [A loss: 0.858639, acc: 0.062500]\n",
      "331: [D loss: 0.679325, acc: 0.558594]: [A loss: 0.781042, acc: 0.195312]\n",
      "332: [D loss: 0.680032, acc: 0.539062]: [A loss: 0.834654, acc: 0.136719]\n",
      "333: [D loss: 0.684003, acc: 0.550781]: [A loss: 0.883453, acc: 0.035156]\n",
      "334: [D loss: 0.669496, acc: 0.593750]: [A loss: 0.783973, acc: 0.199219]\n",
      "335: [D loss: 0.683746, acc: 0.544922]: [A loss: 0.928866, acc: 0.031250]\n",
      "336: [D loss: 0.664927, acc: 0.632812]: [A loss: 0.762154, acc: 0.257812]\n",
      "337: [D loss: 0.697990, acc: 0.523438]: [A loss: 1.049991, acc: 0.000000]\n",
      "338: [D loss: 0.690260, acc: 0.525391]: [A loss: 0.628118, acc: 0.726562]\n",
      "339: [D loss: 0.713528, acc: 0.507812]: [A loss: 1.105887, acc: 0.000000]\n",
      "340: [D loss: 0.698849, acc: 0.525391]: [A loss: 0.605250, acc: 0.789062]\n",
      "341: [D loss: 0.728842, acc: 0.500000]: [A loss: 1.022939, acc: 0.000000]\n",
      "342: [D loss: 0.683712, acc: 0.525391]: [A loss: 0.672960, acc: 0.601562]\n",
      "343: [D loss: 0.705318, acc: 0.513672]: [A loss: 0.928998, acc: 0.015625]\n",
      "344: [D loss: 0.676507, acc: 0.576172]: [A loss: 0.715222, acc: 0.410156]\n",
      "345: [D loss: 0.704355, acc: 0.507812]: [A loss: 0.916057, acc: 0.007812]\n",
      "346: [D loss: 0.686807, acc: 0.519531]: [A loss: 0.728369, acc: 0.386719]\n",
      "347: [D loss: 0.697836, acc: 0.517578]: [A loss: 0.931502, acc: 0.023438]\n",
      "348: [D loss: 0.684649, acc: 0.568359]: [A loss: 0.737390, acc: 0.332031]\n",
      "349: [D loss: 0.699079, acc: 0.507812]: [A loss: 0.881537, acc: 0.050781]\n",
      "350: [D loss: 0.673541, acc: 0.583984]: [A loss: 0.738573, acc: 0.332031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351: [D loss: 0.685457, acc: 0.537109]: [A loss: 0.902829, acc: 0.027344]\n",
      "352: [D loss: 0.674084, acc: 0.605469]: [A loss: 0.732899, acc: 0.363281]\n",
      "353: [D loss: 0.693807, acc: 0.527344]: [A loss: 0.976605, acc: 0.011719]\n",
      "354: [D loss: 0.681400, acc: 0.570312]: [A loss: 0.723798, acc: 0.417969]\n",
      "355: [D loss: 0.702019, acc: 0.505859]: [A loss: 1.022209, acc: 0.003906]\n",
      "356: [D loss: 0.677608, acc: 0.580078]: [A loss: 0.617785, acc: 0.808594]\n",
      "357: [D loss: 0.724390, acc: 0.500000]: [A loss: 1.032721, acc: 0.000000]\n",
      "358: [D loss: 0.689773, acc: 0.501953]: [A loss: 0.677677, acc: 0.582031]\n",
      "359: [D loss: 0.704707, acc: 0.503906]: [A loss: 0.893203, acc: 0.015625]\n",
      "360: [D loss: 0.677230, acc: 0.574219]: [A loss: 0.743706, acc: 0.320312]\n",
      "361: [D loss: 0.693072, acc: 0.525391]: [A loss: 0.909301, acc: 0.011719]\n",
      "362: [D loss: 0.672695, acc: 0.587891]: [A loss: 0.721467, acc: 0.355469]\n",
      "363: [D loss: 0.695457, acc: 0.525391]: [A loss: 0.875693, acc: 0.046875]\n",
      "364: [D loss: 0.687473, acc: 0.542969]: [A loss: 0.755456, acc: 0.257812]\n",
      "365: [D loss: 0.689773, acc: 0.507812]: [A loss: 0.921118, acc: 0.011719]\n",
      "366: [D loss: 0.681403, acc: 0.552734]: [A loss: 0.716151, acc: 0.382812]\n",
      "367: [D loss: 0.701785, acc: 0.505859]: [A loss: 0.947540, acc: 0.011719]\n",
      "368: [D loss: 0.676839, acc: 0.578125]: [A loss: 0.659310, acc: 0.625000]\n",
      "369: [D loss: 0.707088, acc: 0.511719]: [A loss: 0.983210, acc: 0.007812]\n",
      "370: [D loss: 0.685965, acc: 0.525391]: [A loss: 0.671203, acc: 0.613281]\n",
      "371: [D loss: 0.714683, acc: 0.492188]: [A loss: 0.902410, acc: 0.031250]\n",
      "372: [D loss: 0.681672, acc: 0.556641]: [A loss: 0.699508, acc: 0.445312]\n",
      "373: [D loss: 0.695178, acc: 0.529297]: [A loss: 0.889219, acc: 0.023438]\n",
      "374: [D loss: 0.692817, acc: 0.541016]: [A loss: 0.718808, acc: 0.390625]\n",
      "375: [D loss: 0.688768, acc: 0.537109]: [A loss: 0.894810, acc: 0.027344]\n",
      "376: [D loss: 0.682726, acc: 0.574219]: [A loss: 0.712850, acc: 0.421875]\n",
      "377: [D loss: 0.699262, acc: 0.515625]: [A loss: 0.933766, acc: 0.011719]\n",
      "378: [D loss: 0.684530, acc: 0.539062]: [A loss: 0.686304, acc: 0.519531]\n",
      "379: [D loss: 0.695192, acc: 0.507812]: [A loss: 0.953263, acc: 0.015625]\n",
      "380: [D loss: 0.689184, acc: 0.535156]: [A loss: 0.696552, acc: 0.484375]\n",
      "381: [D loss: 0.704781, acc: 0.501953]: [A loss: 0.899744, acc: 0.015625]\n",
      "382: [D loss: 0.681735, acc: 0.537109]: [A loss: 0.701693, acc: 0.453125]\n",
      "383: [D loss: 0.717495, acc: 0.505859]: [A loss: 0.939919, acc: 0.007812]\n",
      "384: [D loss: 0.688161, acc: 0.535156]: [A loss: 0.678603, acc: 0.542969]\n",
      "385: [D loss: 0.696382, acc: 0.511719]: [A loss: 0.883194, acc: 0.023438]\n",
      "386: [D loss: 0.677392, acc: 0.566406]: [A loss: 0.714225, acc: 0.406250]\n",
      "387: [D loss: 0.688795, acc: 0.523438]: [A loss: 0.847694, acc: 0.054688]\n",
      "388: [D loss: 0.676863, acc: 0.578125]: [A loss: 0.753666, acc: 0.230469]\n",
      "389: [D loss: 0.690460, acc: 0.525391]: [A loss: 0.851926, acc: 0.062500]\n",
      "390: [D loss: 0.685897, acc: 0.542969]: [A loss: 0.732129, acc: 0.347656]\n",
      "391: [D loss: 0.695321, acc: 0.529297]: [A loss: 0.871090, acc: 0.042969]\n",
      "392: [D loss: 0.685056, acc: 0.558594]: [A loss: 0.752373, acc: 0.304688]\n",
      "393: [D loss: 0.697018, acc: 0.523438]: [A loss: 0.855980, acc: 0.046875]\n",
      "394: [D loss: 0.674355, acc: 0.611328]: [A loss: 0.767557, acc: 0.238281]\n",
      "395: [D loss: 0.692759, acc: 0.533203]: [A loss: 0.903255, acc: 0.015625]\n",
      "396: [D loss: 0.686955, acc: 0.558594]: [A loss: 0.718834, acc: 0.417969]\n",
      "397: [D loss: 0.700046, acc: 0.509766]: [A loss: 0.982275, acc: 0.000000]\n",
      "398: [D loss: 0.682296, acc: 0.542969]: [A loss: 0.633106, acc: 0.773438]\n",
      "399: [D loss: 0.703970, acc: 0.507812]: [A loss: 0.956814, acc: 0.003906]\n",
      "400: [D loss: 0.681450, acc: 0.546875]: [A loss: 0.644094, acc: 0.679688]\n",
      "401: [D loss: 0.701593, acc: 0.515625]: [A loss: 0.892134, acc: 0.023438]\n",
      "402: [D loss: 0.681907, acc: 0.570312]: [A loss: 0.719755, acc: 0.425781]\n",
      "403: [D loss: 0.713198, acc: 0.496094]: [A loss: 0.852560, acc: 0.054688]\n",
      "404: [D loss: 0.688222, acc: 0.548828]: [A loss: 0.765690, acc: 0.207031]\n",
      "405: [D loss: 0.698195, acc: 0.523438]: [A loss: 0.826328, acc: 0.085938]\n",
      "406: [D loss: 0.694240, acc: 0.501953]: [A loss: 0.777170, acc: 0.148438]\n",
      "407: [D loss: 0.691734, acc: 0.541016]: [A loss: 0.799393, acc: 0.117188]\n",
      "408: [D loss: 0.684354, acc: 0.548828]: [A loss: 0.755636, acc: 0.230469]\n",
      "409: [D loss: 0.691128, acc: 0.556641]: [A loss: 0.822402, acc: 0.089844]\n",
      "410: [D loss: 0.684412, acc: 0.562500]: [A loss: 0.808920, acc: 0.078125]\n",
      "411: [D loss: 0.682473, acc: 0.546875]: [A loss: 0.771907, acc: 0.230469]\n",
      "412: [D loss: 0.695267, acc: 0.511719]: [A loss: 0.876456, acc: 0.042969]\n",
      "413: [D loss: 0.681266, acc: 0.578125]: [A loss: 0.742668, acc: 0.335938]\n",
      "414: [D loss: 0.692601, acc: 0.517578]: [A loss: 0.866118, acc: 0.031250]\n",
      "415: [D loss: 0.682861, acc: 0.566406]: [A loss: 0.755684, acc: 0.230469]\n",
      "416: [D loss: 0.692938, acc: 0.529297]: [A loss: 0.863188, acc: 0.039062]\n",
      "417: [D loss: 0.693094, acc: 0.527344]: [A loss: 0.732061, acc: 0.320312]\n",
      "418: [D loss: 0.699554, acc: 0.527344]: [A loss: 0.868427, acc: 0.046875]\n",
      "419: [D loss: 0.685581, acc: 0.568359]: [A loss: 0.700598, acc: 0.468750]\n",
      "420: [D loss: 0.693807, acc: 0.523438]: [A loss: 0.916977, acc: 0.003906]\n",
      "421: [D loss: 0.689747, acc: 0.525391]: [A loss: 0.693658, acc: 0.511719]\n",
      "422: [D loss: 0.694981, acc: 0.511719]: [A loss: 0.898001, acc: 0.015625]\n",
      "423: [D loss: 0.686511, acc: 0.542969]: [A loss: 0.701340, acc: 0.472656]\n",
      "424: [D loss: 0.693398, acc: 0.509766]: [A loss: 0.830528, acc: 0.054688]\n",
      "425: [D loss: 0.685929, acc: 0.558594]: [A loss: 0.740017, acc: 0.265625]\n",
      "426: [D loss: 0.692275, acc: 0.527344]: [A loss: 0.830312, acc: 0.050781]\n",
      "427: [D loss: 0.685418, acc: 0.564453]: [A loss: 0.734867, acc: 0.347656]\n",
      "428: [D loss: 0.695834, acc: 0.509766]: [A loss: 0.879146, acc: 0.023438]\n",
      "429: [D loss: 0.683131, acc: 0.560547]: [A loss: 0.709286, acc: 0.445312]\n",
      "430: [D loss: 0.687300, acc: 0.527344]: [A loss: 0.840889, acc: 0.062500]\n",
      "431: [D loss: 0.682922, acc: 0.564453]: [A loss: 0.771123, acc: 0.203125]\n",
      "432: [D loss: 0.689364, acc: 0.527344]: [A loss: 0.850550, acc: 0.058594]\n",
      "433: [D loss: 0.686222, acc: 0.525391]: [A loss: 0.710873, acc: 0.429688]\n",
      "434: [D loss: 0.690870, acc: 0.527344]: [A loss: 0.870663, acc: 0.046875]\n",
      "435: [D loss: 0.686119, acc: 0.529297]: [A loss: 0.705482, acc: 0.453125]\n",
      "436: [D loss: 0.693010, acc: 0.513672]: [A loss: 0.903401, acc: 0.011719]\n",
      "437: [D loss: 0.682738, acc: 0.556641]: [A loss: 0.687565, acc: 0.519531]\n",
      "438: [D loss: 0.702749, acc: 0.500000]: [A loss: 0.922011, acc: 0.000000]\n",
      "439: [D loss: 0.686702, acc: 0.525391]: [A loss: 0.678153, acc: 0.613281]\n",
      "440: [D loss: 0.692042, acc: 0.511719]: [A loss: 0.875407, acc: 0.011719]\n",
      "441: [D loss: 0.681745, acc: 0.587891]: [A loss: 0.694512, acc: 0.496094]\n",
      "442: [D loss: 0.694968, acc: 0.507812]: [A loss: 0.875765, acc: 0.011719]\n",
      "443: [D loss: 0.685472, acc: 0.560547]: [A loss: 0.709235, acc: 0.417969]\n",
      "444: [D loss: 0.698069, acc: 0.501953]: [A loss: 0.853949, acc: 0.031250]\n",
      "445: [D loss: 0.688749, acc: 0.535156]: [A loss: 0.728978, acc: 0.292969]\n",
      "446: [D loss: 0.695457, acc: 0.535156]: [A loss: 0.829879, acc: 0.050781]\n",
      "447: [D loss: 0.686761, acc: 0.535156]: [A loss: 0.727092, acc: 0.335938]\n",
      "448: [D loss: 0.686645, acc: 0.513672]: [A loss: 0.802262, acc: 0.085938]\n",
      "449: [D loss: 0.682178, acc: 0.566406]: [A loss: 0.770189, acc: 0.214844]\n",
      "450: [D loss: 0.688974, acc: 0.525391]: [A loss: 0.810855, acc: 0.082031]\n",
      "451: [D loss: 0.685798, acc: 0.542969]: [A loss: 0.780577, acc: 0.140625]\n",
      "452: [D loss: 0.687222, acc: 0.548828]: [A loss: 0.794383, acc: 0.175781]\n",
      "453: [D loss: 0.717422, acc: 0.462891]: [A loss: 0.892770, acc: 0.031250]\n",
      "454: [D loss: 0.688524, acc: 0.531250]: [A loss: 0.716634, acc: 0.402344]\n",
      "455: [D loss: 0.696547, acc: 0.517578]: [A loss: 0.839886, acc: 0.039062]\n",
      "456: [D loss: 0.677535, acc: 0.572266]: [A loss: 0.696820, acc: 0.500000]\n",
      "457: [D loss: 0.698744, acc: 0.515625]: [A loss: 0.857442, acc: 0.011719]\n",
      "458: [D loss: 0.676469, acc: 0.593750]: [A loss: 0.705728, acc: 0.433594]\n",
      "459: [D loss: 0.687718, acc: 0.535156]: [A loss: 0.873543, acc: 0.046875]\n",
      "460: [D loss: 0.690279, acc: 0.539062]: [A loss: 0.725727, acc: 0.359375]\n",
      "461: [D loss: 0.690957, acc: 0.511719]: [A loss: 0.822387, acc: 0.085938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462: [D loss: 0.685580, acc: 0.556641]: [A loss: 0.737530, acc: 0.312500]\n",
      "463: [D loss: 0.685399, acc: 0.541016]: [A loss: 0.829291, acc: 0.082031]\n",
      "464: [D loss: 0.684353, acc: 0.560547]: [A loss: 0.747485, acc: 0.289062]\n",
      "465: [D loss: 0.695347, acc: 0.529297]: [A loss: 0.797985, acc: 0.140625]\n",
      "466: [D loss: 0.687458, acc: 0.548828]: [A loss: 0.801076, acc: 0.148438]\n",
      "467: [D loss: 0.689525, acc: 0.533203]: [A loss: 0.817655, acc: 0.082031]\n",
      "468: [D loss: 0.681792, acc: 0.582031]: [A loss: 0.760970, acc: 0.246094]\n",
      "469: [D loss: 0.694509, acc: 0.537109]: [A loss: 0.870782, acc: 0.035156]\n",
      "470: [D loss: 0.686454, acc: 0.554688]: [A loss: 0.711862, acc: 0.414062]\n",
      "471: [D loss: 0.692424, acc: 0.521484]: [A loss: 0.908837, acc: 0.007812]\n",
      "472: [D loss: 0.688893, acc: 0.531250]: [A loss: 0.696538, acc: 0.464844]\n",
      "473: [D loss: 0.696055, acc: 0.521484]: [A loss: 0.852780, acc: 0.046875]\n",
      "474: [D loss: 0.685897, acc: 0.560547]: [A loss: 0.691926, acc: 0.535156]\n",
      "475: [D loss: 0.712537, acc: 0.494141]: [A loss: 0.887994, acc: 0.011719]\n",
      "476: [D loss: 0.678791, acc: 0.578125]: [A loss: 0.683380, acc: 0.578125]\n",
      "477: [D loss: 0.702150, acc: 0.505859]: [A loss: 0.824205, acc: 0.054688]\n",
      "478: [D loss: 0.681210, acc: 0.574219]: [A loss: 0.711044, acc: 0.421875]\n",
      "479: [D loss: 0.699779, acc: 0.513672]: [A loss: 0.802922, acc: 0.093750]\n",
      "480: [D loss: 0.686626, acc: 0.564453]: [A loss: 0.752191, acc: 0.261719]\n",
      "481: [D loss: 0.693662, acc: 0.509766]: [A loss: 0.783532, acc: 0.121094]\n",
      "482: [D loss: 0.684925, acc: 0.546875]: [A loss: 0.757022, acc: 0.222656]\n",
      "483: [D loss: 0.691826, acc: 0.546875]: [A loss: 0.773145, acc: 0.191406]\n",
      "484: [D loss: 0.685743, acc: 0.535156]: [A loss: 0.767402, acc: 0.230469]\n",
      "485: [D loss: 0.686912, acc: 0.533203]: [A loss: 0.809468, acc: 0.101562]\n",
      "486: [D loss: 0.685278, acc: 0.521484]: [A loss: 0.787700, acc: 0.156250]\n",
      "487: [D loss: 0.685991, acc: 0.550781]: [A loss: 0.799344, acc: 0.156250]\n",
      "488: [D loss: 0.691428, acc: 0.519531]: [A loss: 0.832955, acc: 0.062500]\n",
      "489: [D loss: 0.679691, acc: 0.585938]: [A loss: 0.765219, acc: 0.246094]\n",
      "490: [D loss: 0.696025, acc: 0.527344]: [A loss: 0.839049, acc: 0.062500]\n",
      "491: [D loss: 0.686839, acc: 0.546875]: [A loss: 0.759934, acc: 0.238281]\n",
      "492: [D loss: 0.691240, acc: 0.541016]: [A loss: 0.850307, acc: 0.066406]\n",
      "493: [D loss: 0.684287, acc: 0.541016]: [A loss: 0.723034, acc: 0.347656]\n",
      "494: [D loss: 0.690280, acc: 0.519531]: [A loss: 0.853728, acc: 0.058594]\n",
      "495: [D loss: 0.682431, acc: 0.570312]: [A loss: 0.733141, acc: 0.351562]\n",
      "496: [D loss: 0.692114, acc: 0.509766]: [A loss: 0.868480, acc: 0.031250]\n",
      "497: [D loss: 0.686085, acc: 0.558594]: [A loss: 0.734231, acc: 0.300781]\n",
      "498: [D loss: 0.686269, acc: 0.535156]: [A loss: 0.790156, acc: 0.144531]\n",
      "499: [D loss: 0.684443, acc: 0.554688]: [A loss: 0.774997, acc: 0.148438]\n",
      "500: [D loss: 0.686347, acc: 0.544922]: [A loss: 0.802574, acc: 0.140625]\n",
      "501: [D loss: 0.679401, acc: 0.550781]: [A loss: 0.765355, acc: 0.234375]\n",
      "502: [D loss: 0.685148, acc: 0.541016]: [A loss: 0.823890, acc: 0.113281]\n",
      "503: [D loss: 0.687298, acc: 0.525391]: [A loss: 0.792105, acc: 0.175781]\n",
      "504: [D loss: 0.685088, acc: 0.556641]: [A loss: 0.819676, acc: 0.121094]\n",
      "505: [D loss: 0.691523, acc: 0.535156]: [A loss: 0.849324, acc: 0.062500]\n",
      "506: [D loss: 0.685433, acc: 0.537109]: [A loss: 0.780434, acc: 0.226562]\n",
      "507: [D loss: 0.678875, acc: 0.554688]: [A loss: 0.905553, acc: 0.023438]\n",
      "508: [D loss: 0.681583, acc: 0.576172]: [A loss: 0.713832, acc: 0.429688]\n",
      "509: [D loss: 0.692346, acc: 0.531250]: [A loss: 1.015036, acc: 0.000000]\n",
      "510: [D loss: 0.682749, acc: 0.537109]: [A loss: 0.663442, acc: 0.625000]\n",
      "511: [D loss: 0.703854, acc: 0.505859]: [A loss: 0.915042, acc: 0.011719]\n",
      "512: [D loss: 0.683260, acc: 0.560547]: [A loss: 0.723836, acc: 0.390625]\n",
      "513: [D loss: 0.697071, acc: 0.507812]: [A loss: 0.844254, acc: 0.042969]\n",
      "514: [D loss: 0.684214, acc: 0.523438]: [A loss: 0.748749, acc: 0.257812]\n",
      "515: [D loss: 0.683181, acc: 0.544922]: [A loss: 0.793336, acc: 0.179688]\n",
      "516: [D loss: 0.685915, acc: 0.531250]: [A loss: 0.782762, acc: 0.195312]\n",
      "517: [D loss: 0.683191, acc: 0.544922]: [A loss: 0.777564, acc: 0.210938]\n",
      "518: [D loss: 0.688431, acc: 0.548828]: [A loss: 0.804328, acc: 0.128906]\n",
      "519: [D loss: 0.689189, acc: 0.527344]: [A loss: 0.795467, acc: 0.144531]\n",
      "520: [D loss: 0.692838, acc: 0.513672]: [A loss: 0.827245, acc: 0.078125]\n",
      "521: [D loss: 0.684500, acc: 0.552734]: [A loss: 0.775078, acc: 0.203125]\n",
      "522: [D loss: 0.684389, acc: 0.558594]: [A loss: 0.804475, acc: 0.117188]\n",
      "523: [D loss: 0.692346, acc: 0.537109]: [A loss: 0.796056, acc: 0.183594]\n",
      "524: [D loss: 0.692806, acc: 0.537109]: [A loss: 0.811482, acc: 0.105469]\n",
      "525: [D loss: 0.684349, acc: 0.541016]: [A loss: 0.782338, acc: 0.125000]\n",
      "526: [D loss: 0.686783, acc: 0.556641]: [A loss: 0.840607, acc: 0.082031]\n",
      "527: [D loss: 0.682843, acc: 0.529297]: [A loss: 0.792422, acc: 0.148438]\n",
      "528: [D loss: 0.683706, acc: 0.542969]: [A loss: 0.841220, acc: 0.117188]\n",
      "529: [D loss: 0.692810, acc: 0.531250]: [A loss: 0.793002, acc: 0.164062]\n",
      "530: [D loss: 0.680477, acc: 0.578125]: [A loss: 0.831483, acc: 0.101562]\n",
      "531: [D loss: 0.682642, acc: 0.544922]: [A loss: 0.817404, acc: 0.148438]\n",
      "532: [D loss: 0.684740, acc: 0.556641]: [A loss: 0.869216, acc: 0.042969]\n",
      "533: [D loss: 0.689297, acc: 0.541016]: [A loss: 0.755951, acc: 0.281250]\n",
      "534: [D loss: 0.694670, acc: 0.537109]: [A loss: 0.892371, acc: 0.027344]\n",
      "535: [D loss: 0.680803, acc: 0.585938]: [A loss: 0.708587, acc: 0.402344]\n",
      "536: [D loss: 0.698938, acc: 0.503906]: [A loss: 0.958712, acc: 0.011719]\n",
      "537: [D loss: 0.675456, acc: 0.585938]: [A loss: 0.662201, acc: 0.632812]\n",
      "538: [D loss: 0.701067, acc: 0.517578]: [A loss: 0.934677, acc: 0.011719]\n",
      "539: [D loss: 0.678573, acc: 0.572266]: [A loss: 0.701808, acc: 0.468750]\n",
      "540: [D loss: 0.694926, acc: 0.521484]: [A loss: 0.896532, acc: 0.031250]\n",
      "541: [D loss: 0.686787, acc: 0.519531]: [A loss: 0.742176, acc: 0.300781]\n",
      "542: [D loss: 0.683161, acc: 0.550781]: [A loss: 0.832557, acc: 0.070312]\n",
      "543: [D loss: 0.687162, acc: 0.513672]: [A loss: 0.763227, acc: 0.242188]\n",
      "544: [D loss: 0.699212, acc: 0.527344]: [A loss: 0.812468, acc: 0.144531]\n",
      "545: [D loss: 0.684940, acc: 0.576172]: [A loss: 0.756470, acc: 0.312500]\n",
      "546: [D loss: 0.699056, acc: 0.513672]: [A loss: 0.829072, acc: 0.132812]\n",
      "547: [D loss: 0.678052, acc: 0.550781]: [A loss: 0.736553, acc: 0.343750]\n",
      "548: [D loss: 0.694296, acc: 0.550781]: [A loss: 0.862458, acc: 0.058594]\n",
      "549: [D loss: 0.691253, acc: 0.537109]: [A loss: 0.719130, acc: 0.429688]\n",
      "550: [D loss: 0.710151, acc: 0.494141]: [A loss: 0.883129, acc: 0.035156]\n",
      "551: [D loss: 0.685570, acc: 0.527344]: [A loss: 0.726091, acc: 0.378906]\n",
      "552: [D loss: 0.693749, acc: 0.527344]: [A loss: 0.830425, acc: 0.066406]\n",
      "553: [D loss: 0.678239, acc: 0.574219]: [A loss: 0.743106, acc: 0.300781]\n",
      "554: [D loss: 0.691192, acc: 0.531250]: [A loss: 0.826235, acc: 0.101562]\n",
      "555: [D loss: 0.686011, acc: 0.558594]: [A loss: 0.750418, acc: 0.320312]\n",
      "556: [D loss: 0.699534, acc: 0.517578]: [A loss: 0.831796, acc: 0.093750]\n",
      "557: [D loss: 0.686701, acc: 0.521484]: [A loss: 0.762498, acc: 0.246094]\n",
      "558: [D loss: 0.686747, acc: 0.539062]: [A loss: 0.803327, acc: 0.140625]\n",
      "559: [D loss: 0.687783, acc: 0.539062]: [A loss: 0.778343, acc: 0.226562]\n",
      "560: [D loss: 0.688651, acc: 0.552734]: [A loss: 0.825873, acc: 0.089844]\n",
      "561: [D loss: 0.688303, acc: 0.544922]: [A loss: 0.755417, acc: 0.257812]\n",
      "562: [D loss: 0.689798, acc: 0.523438]: [A loss: 0.837155, acc: 0.144531]\n",
      "563: [D loss: 0.689318, acc: 0.548828]: [A loss: 0.813547, acc: 0.113281]\n",
      "564: [D loss: 0.683155, acc: 0.564453]: [A loss: 0.763230, acc: 0.242188]\n",
      "565: [D loss: 0.680869, acc: 0.566406]: [A loss: 0.838594, acc: 0.093750]\n",
      "566: [D loss: 0.681650, acc: 0.542969]: [A loss: 0.784666, acc: 0.207031]\n",
      "567: [D loss: 0.686477, acc: 0.537109]: [A loss: 0.845152, acc: 0.109375]\n",
      "568: [D loss: 0.687376, acc: 0.541016]: [A loss: 0.750111, acc: 0.312500]\n",
      "569: [D loss: 0.690494, acc: 0.539062]: [A loss: 0.853695, acc: 0.093750]\n",
      "570: [D loss: 0.680741, acc: 0.576172]: [A loss: 0.729469, acc: 0.386719]\n",
      "571: [D loss: 0.698450, acc: 0.542969]: [A loss: 0.820257, acc: 0.113281]\n",
      "572: [D loss: 0.695147, acc: 0.488281]: [A loss: 0.793268, acc: 0.183594]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573: [D loss: 0.687071, acc: 0.529297]: [A loss: 0.846540, acc: 0.105469]\n",
      "574: [D loss: 0.692351, acc: 0.519531]: [A loss: 0.802507, acc: 0.152344]\n",
      "575: [D loss: 0.686077, acc: 0.531250]: [A loss: 0.760164, acc: 0.265625]\n",
      "576: [D loss: 0.688873, acc: 0.546875]: [A loss: 0.812052, acc: 0.117188]\n",
      "577: [D loss: 0.682674, acc: 0.583984]: [A loss: 0.795983, acc: 0.164062]\n",
      "578: [D loss: 0.681169, acc: 0.566406]: [A loss: 0.777574, acc: 0.242188]\n",
      "579: [D loss: 0.688647, acc: 0.535156]: [A loss: 0.885485, acc: 0.027344]\n",
      "580: [D loss: 0.686591, acc: 0.542969]: [A loss: 0.693428, acc: 0.500000]\n",
      "581: [D loss: 0.695003, acc: 0.513672]: [A loss: 0.924701, acc: 0.015625]\n",
      "582: [D loss: 0.680775, acc: 0.564453]: [A loss: 0.674212, acc: 0.597656]\n",
      "583: [D loss: 0.693031, acc: 0.515625]: [A loss: 0.923008, acc: 0.015625]\n",
      "584: [D loss: 0.688293, acc: 0.535156]: [A loss: 0.692394, acc: 0.515625]\n",
      "585: [D loss: 0.698861, acc: 0.519531]: [A loss: 0.834053, acc: 0.109375]\n",
      "586: [D loss: 0.676628, acc: 0.564453]: [A loss: 0.749719, acc: 0.308594]\n",
      "587: [D loss: 0.687354, acc: 0.544922]: [A loss: 0.800012, acc: 0.171875]\n",
      "588: [D loss: 0.687827, acc: 0.570312]: [A loss: 0.801118, acc: 0.136719]\n",
      "589: [D loss: 0.680798, acc: 0.550781]: [A loss: 0.763708, acc: 0.261719]\n",
      "590: [D loss: 0.689307, acc: 0.541016]: [A loss: 0.855495, acc: 0.093750]\n",
      "591: [D loss: 0.678754, acc: 0.580078]: [A loss: 0.729656, acc: 0.406250]\n",
      "592: [D loss: 0.701018, acc: 0.515625]: [A loss: 0.883429, acc: 0.039062]\n",
      "593: [D loss: 0.682300, acc: 0.572266]: [A loss: 0.716255, acc: 0.437500]\n",
      "594: [D loss: 0.699316, acc: 0.517578]: [A loss: 0.826745, acc: 0.117188]\n",
      "595: [D loss: 0.686963, acc: 0.527344]: [A loss: 0.737037, acc: 0.363281]\n",
      "596: [D loss: 0.696270, acc: 0.503906]: [A loss: 0.800447, acc: 0.121094]\n",
      "597: [D loss: 0.688878, acc: 0.541016]: [A loss: 0.783378, acc: 0.171875]\n",
      "598: [D loss: 0.689694, acc: 0.519531]: [A loss: 0.808956, acc: 0.128906]\n",
      "599: [D loss: 0.692984, acc: 0.523438]: [A loss: 0.765179, acc: 0.257812]\n",
      "600: [D loss: 0.684361, acc: 0.537109]: [A loss: 0.760326, acc: 0.363281]\n",
      "601: [D loss: 0.702854, acc: 0.498047]: [A loss: 0.828120, acc: 0.085938]\n",
      "602: [D loss: 0.682628, acc: 0.541016]: [A loss: 0.755962, acc: 0.277344]\n",
      "603: [D loss: 0.690974, acc: 0.533203]: [A loss: 0.886969, acc: 0.046875]\n",
      "604: [D loss: 0.682818, acc: 0.558594]: [A loss: 0.729906, acc: 0.363281]\n",
      "605: [D loss: 0.697525, acc: 0.523438]: [A loss: 0.877412, acc: 0.039062]\n",
      "606: [D loss: 0.686026, acc: 0.548828]: [A loss: 0.715659, acc: 0.472656]\n",
      "607: [D loss: 0.698766, acc: 0.498047]: [A loss: 0.883173, acc: 0.027344]\n",
      "608: [D loss: 0.686335, acc: 0.550781]: [A loss: 0.719263, acc: 0.406250]\n",
      "609: [D loss: 0.686079, acc: 0.556641]: [A loss: 0.841188, acc: 0.082031]\n",
      "610: [D loss: 0.682779, acc: 0.562500]: [A loss: 0.740726, acc: 0.343750]\n",
      "611: [D loss: 0.689626, acc: 0.533203]: [A loss: 0.843396, acc: 0.089844]\n",
      "612: [D loss: 0.683526, acc: 0.556641]: [A loss: 0.756067, acc: 0.328125]\n",
      "613: [D loss: 0.689734, acc: 0.537109]: [A loss: 0.834789, acc: 0.113281]\n",
      "614: [D loss: 0.687444, acc: 0.511719]: [A loss: 0.777647, acc: 0.218750]\n",
      "615: [D loss: 0.686357, acc: 0.548828]: [A loss: 0.853789, acc: 0.082031]\n",
      "616: [D loss: 0.690241, acc: 0.523438]: [A loss: 0.753689, acc: 0.273438]\n",
      "617: [D loss: 0.689301, acc: 0.535156]: [A loss: 0.834450, acc: 0.113281]\n",
      "618: [D loss: 0.678427, acc: 0.587891]: [A loss: 0.728404, acc: 0.371094]\n",
      "619: [D loss: 0.693184, acc: 0.542969]: [A loss: 0.849621, acc: 0.082031]\n",
      "620: [D loss: 0.691775, acc: 0.529297]: [A loss: 0.691141, acc: 0.492188]\n",
      "621: [D loss: 0.700439, acc: 0.501953]: [A loss: 0.870350, acc: 0.027344]\n",
      "622: [D loss: 0.690621, acc: 0.548828]: [A loss: 0.716559, acc: 0.394531]\n",
      "623: [D loss: 0.690235, acc: 0.529297]: [A loss: 0.792094, acc: 0.128906]\n",
      "624: [D loss: 0.688317, acc: 0.556641]: [A loss: 0.769761, acc: 0.187500]\n",
      "625: [D loss: 0.682120, acc: 0.566406]: [A loss: 0.788902, acc: 0.203125]\n",
      "626: [D loss: 0.688055, acc: 0.529297]: [A loss: 0.793104, acc: 0.144531]\n",
      "627: [D loss: 0.688641, acc: 0.517578]: [A loss: 0.739937, acc: 0.308594]\n",
      "628: [D loss: 0.691876, acc: 0.490234]: [A loss: 0.811448, acc: 0.121094]\n",
      "629: [D loss: 0.681119, acc: 0.558594]: [A loss: 0.770174, acc: 0.242188]\n",
      "630: [D loss: 0.692809, acc: 0.558594]: [A loss: 0.828606, acc: 0.128906]\n",
      "631: [D loss: 0.695213, acc: 0.501953]: [A loss: 0.720042, acc: 0.382812]\n",
      "632: [D loss: 0.700134, acc: 0.517578]: [A loss: 0.842177, acc: 0.066406]\n",
      "633: [D loss: 0.691247, acc: 0.548828]: [A loss: 0.729968, acc: 0.347656]\n",
      "634: [D loss: 0.682175, acc: 0.572266]: [A loss: 0.834577, acc: 0.089844]\n",
      "635: [D loss: 0.683985, acc: 0.533203]: [A loss: 0.746287, acc: 0.308594]\n",
      "636: [D loss: 0.688453, acc: 0.515625]: [A loss: 0.826784, acc: 0.136719]\n",
      "637: [D loss: 0.682404, acc: 0.572266]: [A loss: 0.757880, acc: 0.296875]\n",
      "638: [D loss: 0.688256, acc: 0.542969]: [A loss: 0.814244, acc: 0.105469]\n",
      "639: [D loss: 0.683251, acc: 0.527344]: [A loss: 0.769358, acc: 0.250000]\n",
      "640: [D loss: 0.683774, acc: 0.544922]: [A loss: 0.858807, acc: 0.082031]\n",
      "641: [D loss: 0.681839, acc: 0.544922]: [A loss: 0.734828, acc: 0.300781]\n",
      "642: [D loss: 0.698199, acc: 0.517578]: [A loss: 0.933445, acc: 0.023438]\n",
      "643: [D loss: 0.680891, acc: 0.578125]: [A loss: 0.671911, acc: 0.566406]\n",
      "644: [D loss: 0.700540, acc: 0.527344]: [A loss: 0.915662, acc: 0.039062]\n",
      "645: [D loss: 0.677208, acc: 0.585938]: [A loss: 0.684120, acc: 0.562500]\n",
      "646: [D loss: 0.697489, acc: 0.505859]: [A loss: 0.869014, acc: 0.062500]\n",
      "647: [D loss: 0.683148, acc: 0.546875]: [A loss: 0.722463, acc: 0.402344]\n",
      "648: [D loss: 0.689022, acc: 0.539062]: [A loss: 0.821321, acc: 0.125000]\n",
      "649: [D loss: 0.682641, acc: 0.544922]: [A loss: 0.751075, acc: 0.234375]\n",
      "650: [D loss: 0.687699, acc: 0.535156]: [A loss: 0.805202, acc: 0.125000]\n",
      "651: [D loss: 0.682871, acc: 0.558594]: [A loss: 0.745632, acc: 0.324219]\n",
      "652: [D loss: 0.688036, acc: 0.542969]: [A loss: 0.816217, acc: 0.113281]\n",
      "653: [D loss: 0.683865, acc: 0.556641]: [A loss: 0.779723, acc: 0.226562]\n",
      "654: [D loss: 0.675698, acc: 0.562500]: [A loss: 0.830827, acc: 0.148438]\n",
      "655: [D loss: 0.687778, acc: 0.562500]: [A loss: 0.782851, acc: 0.222656]\n",
      "656: [D loss: 0.672728, acc: 0.597656]: [A loss: 0.832856, acc: 0.183594]\n",
      "657: [D loss: 0.693023, acc: 0.511719]: [A loss: 0.796800, acc: 0.210938]\n",
      "658: [D loss: 0.697433, acc: 0.503906]: [A loss: 0.825075, acc: 0.121094]\n",
      "659: [D loss: 0.688047, acc: 0.560547]: [A loss: 0.728413, acc: 0.371094]\n",
      "660: [D loss: 0.695170, acc: 0.519531]: [A loss: 0.905428, acc: 0.046875]\n",
      "661: [D loss: 0.675272, acc: 0.566406]: [A loss: 0.714659, acc: 0.417969]\n",
      "662: [D loss: 0.693220, acc: 0.511719]: [A loss: 0.894998, acc: 0.046875]\n",
      "663: [D loss: 0.680604, acc: 0.572266]: [A loss: 0.704090, acc: 0.511719]\n",
      "664: [D loss: 0.701441, acc: 0.517578]: [A loss: 0.861109, acc: 0.089844]\n",
      "665: [D loss: 0.688877, acc: 0.511719]: [A loss: 0.732030, acc: 0.343750]\n",
      "666: [D loss: 0.693600, acc: 0.519531]: [A loss: 0.821807, acc: 0.140625]\n",
      "667: [D loss: 0.678072, acc: 0.556641]: [A loss: 0.732556, acc: 0.378906]\n",
      "668: [D loss: 0.694639, acc: 0.523438]: [A loss: 0.838415, acc: 0.078125]\n",
      "669: [D loss: 0.690960, acc: 0.554688]: [A loss: 0.723897, acc: 0.386719]\n",
      "670: [D loss: 0.689708, acc: 0.546875]: [A loss: 0.843191, acc: 0.070312]\n",
      "671: [D loss: 0.685199, acc: 0.548828]: [A loss: 0.763012, acc: 0.265625]\n",
      "672: [D loss: 0.697300, acc: 0.527344]: [A loss: 0.857929, acc: 0.082031]\n",
      "673: [D loss: 0.690888, acc: 0.503906]: [A loss: 0.728277, acc: 0.390625]\n",
      "674: [D loss: 0.684298, acc: 0.542969]: [A loss: 0.822633, acc: 0.140625]\n",
      "675: [D loss: 0.680074, acc: 0.562500]: [A loss: 0.733635, acc: 0.367188]\n",
      "676: [D loss: 0.680164, acc: 0.558594]: [A loss: 0.855737, acc: 0.082031]\n",
      "677: [D loss: 0.677449, acc: 0.591797]: [A loss: 0.726246, acc: 0.390625]\n",
      "678: [D loss: 0.703179, acc: 0.517578]: [A loss: 0.888601, acc: 0.054688]\n",
      "679: [D loss: 0.675321, acc: 0.576172]: [A loss: 0.725319, acc: 0.375000]\n",
      "680: [D loss: 0.701292, acc: 0.515625]: [A loss: 0.868144, acc: 0.050781]\n",
      "681: [D loss: 0.680402, acc: 0.583984]: [A loss: 0.731972, acc: 0.402344]\n",
      "682: [D loss: 0.691027, acc: 0.527344]: [A loss: 0.820505, acc: 0.136719]\n",
      "683: [D loss: 0.679832, acc: 0.572266]: [A loss: 0.745085, acc: 0.367188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "684: [D loss: 0.701947, acc: 0.503906]: [A loss: 0.889196, acc: 0.066406]\n",
      "685: [D loss: 0.680076, acc: 0.574219]: [A loss: 0.681094, acc: 0.554688]\n",
      "686: [D loss: 0.706505, acc: 0.501953]: [A loss: 0.873170, acc: 0.066406]\n",
      "687: [D loss: 0.687819, acc: 0.539062]: [A loss: 0.704667, acc: 0.441406]\n",
      "688: [D loss: 0.686785, acc: 0.541016]: [A loss: 0.829046, acc: 0.132812]\n",
      "689: [D loss: 0.685012, acc: 0.548828]: [A loss: 0.720837, acc: 0.402344]\n",
      "690: [D loss: 0.697545, acc: 0.513672]: [A loss: 0.809816, acc: 0.113281]\n",
      "691: [D loss: 0.687697, acc: 0.527344]: [A loss: 0.736254, acc: 0.343750]\n",
      "692: [D loss: 0.698169, acc: 0.509766]: [A loss: 0.798012, acc: 0.167969]\n",
      "693: [D loss: 0.686019, acc: 0.558594]: [A loss: 0.754256, acc: 0.277344]\n",
      "694: [D loss: 0.702906, acc: 0.544922]: [A loss: 0.816844, acc: 0.136719]\n",
      "695: [D loss: 0.682364, acc: 0.533203]: [A loss: 0.716621, acc: 0.394531]\n",
      "696: [D loss: 0.689714, acc: 0.537109]: [A loss: 0.800410, acc: 0.171875]\n",
      "697: [D loss: 0.686888, acc: 0.541016]: [A loss: 0.780867, acc: 0.226562]\n",
      "698: [D loss: 0.693753, acc: 0.517578]: [A loss: 0.771597, acc: 0.242188]\n",
      "699: [D loss: 0.691291, acc: 0.542969]: [A loss: 0.788254, acc: 0.160156]\n",
      "700: [D loss: 0.685965, acc: 0.556641]: [A loss: 0.777415, acc: 0.230469]\n",
      "701: [D loss: 0.689341, acc: 0.527344]: [A loss: 0.770641, acc: 0.191406]\n",
      "702: [D loss: 0.685907, acc: 0.544922]: [A loss: 0.801977, acc: 0.183594]\n",
      "703: [D loss: 0.693036, acc: 0.519531]: [A loss: 0.783261, acc: 0.207031]\n",
      "704: [D loss: 0.696710, acc: 0.542969]: [A loss: 0.809546, acc: 0.140625]\n",
      "705: [D loss: 0.692733, acc: 0.513672]: [A loss: 0.750793, acc: 0.281250]\n",
      "706: [D loss: 0.691333, acc: 0.527344]: [A loss: 0.827525, acc: 0.105469]\n",
      "707: [D loss: 0.681155, acc: 0.562500]: [A loss: 0.746815, acc: 0.289062]\n",
      "708: [D loss: 0.683990, acc: 0.585938]: [A loss: 0.796999, acc: 0.183594]\n",
      "709: [D loss: 0.686021, acc: 0.564453]: [A loss: 0.754923, acc: 0.300781]\n",
      "710: [D loss: 0.694108, acc: 0.517578]: [A loss: 0.869381, acc: 0.035156]\n",
      "711: [D loss: 0.695416, acc: 0.498047]: [A loss: 0.713810, acc: 0.425781]\n",
      "712: [D loss: 0.693103, acc: 0.529297]: [A loss: 0.799165, acc: 0.152344]\n",
      "713: [D loss: 0.686807, acc: 0.548828]: [A loss: 0.751854, acc: 0.292969]\n",
      "714: [D loss: 0.683851, acc: 0.529297]: [A loss: 0.807682, acc: 0.140625]\n",
      "715: [D loss: 0.688450, acc: 0.527344]: [A loss: 0.753652, acc: 0.257812]\n",
      "716: [D loss: 0.695557, acc: 0.519531]: [A loss: 0.853706, acc: 0.054688]\n",
      "717: [D loss: 0.677685, acc: 0.583984]: [A loss: 0.686598, acc: 0.511719]\n",
      "718: [D loss: 0.699198, acc: 0.505859]: [A loss: 0.909759, acc: 0.023438]\n",
      "719: [D loss: 0.684232, acc: 0.521484]: [A loss: 0.677496, acc: 0.558594]\n",
      "720: [D loss: 0.702451, acc: 0.517578]: [A loss: 0.889169, acc: 0.062500]\n",
      "721: [D loss: 0.688897, acc: 0.539062]: [A loss: 0.713967, acc: 0.425781]\n",
      "722: [D loss: 0.694928, acc: 0.527344]: [A loss: 0.810279, acc: 0.132812]\n",
      "723: [D loss: 0.683821, acc: 0.558594]: [A loss: 0.732595, acc: 0.363281]\n",
      "724: [D loss: 0.698263, acc: 0.529297]: [A loss: 0.801777, acc: 0.136719]\n",
      "725: [D loss: 0.692011, acc: 0.539062]: [A loss: 0.755343, acc: 0.304688]\n",
      "726: [D loss: 0.692679, acc: 0.527344]: [A loss: 0.772521, acc: 0.238281]\n",
      "727: [D loss: 0.686373, acc: 0.525391]: [A loss: 0.761527, acc: 0.273438]\n",
      "728: [D loss: 0.689678, acc: 0.525391]: [A loss: 0.796147, acc: 0.183594]\n",
      "729: [D loss: 0.684082, acc: 0.572266]: [A loss: 0.795391, acc: 0.203125]\n",
      "730: [D loss: 0.692267, acc: 0.501953]: [A loss: 0.820498, acc: 0.105469]\n",
      "731: [D loss: 0.681318, acc: 0.568359]: [A loss: 0.729422, acc: 0.347656]\n",
      "732: [D loss: 0.690752, acc: 0.525391]: [A loss: 0.861213, acc: 0.082031]\n",
      "733: [D loss: 0.680980, acc: 0.558594]: [A loss: 0.763596, acc: 0.277344]\n",
      "734: [D loss: 0.691642, acc: 0.572266]: [A loss: 0.803835, acc: 0.175781]\n",
      "735: [D loss: 0.673962, acc: 0.615234]: [A loss: 0.741124, acc: 0.359375]\n",
      "736: [D loss: 0.688756, acc: 0.525391]: [A loss: 0.830229, acc: 0.140625]\n",
      "737: [D loss: 0.680772, acc: 0.576172]: [A loss: 0.806013, acc: 0.187500]\n",
      "738: [D loss: 0.698531, acc: 0.523438]: [A loss: 0.806103, acc: 0.203125]\n",
      "739: [D loss: 0.684946, acc: 0.560547]: [A loss: 0.770545, acc: 0.230469]\n",
      "740: [D loss: 0.699852, acc: 0.511719]: [A loss: 0.826880, acc: 0.101562]\n",
      "741: [D loss: 0.683566, acc: 0.558594]: [A loss: 0.742856, acc: 0.335938]\n",
      "742: [D loss: 0.695956, acc: 0.544922]: [A loss: 0.860597, acc: 0.093750]\n",
      "743: [D loss: 0.679880, acc: 0.572266]: [A loss: 0.702954, acc: 0.492188]\n",
      "744: [D loss: 0.706179, acc: 0.519531]: [A loss: 0.888306, acc: 0.054688]\n",
      "745: [D loss: 0.682306, acc: 0.562500]: [A loss: 0.716817, acc: 0.402344]\n",
      "746: [D loss: 0.698682, acc: 0.527344]: [A loss: 0.854983, acc: 0.078125]\n",
      "747: [D loss: 0.683545, acc: 0.576172]: [A loss: 0.704563, acc: 0.480469]\n",
      "748: [D loss: 0.700070, acc: 0.521484]: [A loss: 0.824159, acc: 0.140625]\n",
      "749: [D loss: 0.700170, acc: 0.494141]: [A loss: 0.751169, acc: 0.296875]\n",
      "750: [D loss: 0.695668, acc: 0.517578]: [A loss: 0.782417, acc: 0.203125]\n",
      "751: [D loss: 0.685429, acc: 0.546875]: [A loss: 0.758753, acc: 0.296875]\n",
      "752: [D loss: 0.699803, acc: 0.535156]: [A loss: 0.817989, acc: 0.121094]\n",
      "753: [D loss: 0.692308, acc: 0.509766]: [A loss: 0.763177, acc: 0.281250]\n",
      "754: [D loss: 0.691897, acc: 0.517578]: [A loss: 0.783965, acc: 0.226562]\n",
      "755: [D loss: 0.688976, acc: 0.535156]: [A loss: 0.785047, acc: 0.222656]\n",
      "756: [D loss: 0.684766, acc: 0.570312]: [A loss: 0.798237, acc: 0.171875]\n",
      "757: [D loss: 0.689786, acc: 0.537109]: [A loss: 0.756059, acc: 0.289062]\n",
      "758: [D loss: 0.692883, acc: 0.537109]: [A loss: 0.775905, acc: 0.242188]\n",
      "759: [D loss: 0.677593, acc: 0.593750]: [A loss: 0.779644, acc: 0.238281]\n",
      "760: [D loss: 0.679176, acc: 0.576172]: [A loss: 0.808773, acc: 0.226562]\n",
      "761: [D loss: 0.691637, acc: 0.515625]: [A loss: 0.784571, acc: 0.210938]\n",
      "762: [D loss: 0.689800, acc: 0.511719]: [A loss: 0.804994, acc: 0.183594]\n",
      "763: [D loss: 0.690717, acc: 0.537109]: [A loss: 0.805682, acc: 0.187500]\n",
      "764: [D loss: 0.690954, acc: 0.533203]: [A loss: 0.788963, acc: 0.207031]\n",
      "765: [D loss: 0.696004, acc: 0.542969]: [A loss: 0.814864, acc: 0.152344]\n",
      "766: [D loss: 0.679214, acc: 0.572266]: [A loss: 0.729195, acc: 0.406250]\n",
      "767: [D loss: 0.690400, acc: 0.548828]: [A loss: 0.843202, acc: 0.117188]\n",
      "768: [D loss: 0.690845, acc: 0.521484]: [A loss: 0.715237, acc: 0.425781]\n",
      "769: [D loss: 0.698357, acc: 0.531250]: [A loss: 0.872206, acc: 0.070312]\n",
      "770: [D loss: 0.687624, acc: 0.542969]: [A loss: 0.705830, acc: 0.453125]\n",
      "771: [D loss: 0.703093, acc: 0.519531]: [A loss: 0.868467, acc: 0.046875]\n",
      "772: [D loss: 0.684938, acc: 0.562500]: [A loss: 0.683122, acc: 0.562500]\n",
      "773: [D loss: 0.700023, acc: 0.523438]: [A loss: 0.874371, acc: 0.058594]\n",
      "774: [D loss: 0.689192, acc: 0.542969]: [A loss: 0.680437, acc: 0.535156]\n",
      "775: [D loss: 0.698626, acc: 0.537109]: [A loss: 0.847174, acc: 0.109375]\n",
      "776: [D loss: 0.692669, acc: 0.519531]: [A loss: 0.718634, acc: 0.394531]\n",
      "777: [D loss: 0.703706, acc: 0.486328]: [A loss: 0.805193, acc: 0.152344]\n",
      "778: [D loss: 0.686710, acc: 0.517578]: [A loss: 0.755784, acc: 0.285156]\n",
      "779: [D loss: 0.697563, acc: 0.513672]: [A loss: 0.758138, acc: 0.257812]\n",
      "780: [D loss: 0.689933, acc: 0.550781]: [A loss: 0.731774, acc: 0.375000]\n",
      "781: [D loss: 0.698913, acc: 0.541016]: [A loss: 0.802576, acc: 0.183594]\n",
      "782: [D loss: 0.692116, acc: 0.525391]: [A loss: 0.763116, acc: 0.250000]\n",
      "783: [D loss: 0.690646, acc: 0.527344]: [A loss: 0.756799, acc: 0.214844]\n",
      "784: [D loss: 0.689560, acc: 0.539062]: [A loss: 0.783905, acc: 0.167969]\n",
      "785: [D loss: 0.685912, acc: 0.564453]: [A loss: 0.730214, acc: 0.382812]\n",
      "786: [D loss: 0.694107, acc: 0.541016]: [A loss: 0.831840, acc: 0.109375]\n",
      "787: [D loss: 0.691509, acc: 0.523438]: [A loss: 0.731288, acc: 0.386719]\n",
      "788: [D loss: 0.703066, acc: 0.507812]: [A loss: 0.816881, acc: 0.117188]\n",
      "789: [D loss: 0.692998, acc: 0.503906]: [A loss: 0.737769, acc: 0.320312]\n",
      "790: [D loss: 0.686152, acc: 0.558594]: [A loss: 0.820544, acc: 0.156250]\n",
      "791: [D loss: 0.696620, acc: 0.515625]: [A loss: 0.730253, acc: 0.347656]\n",
      "792: [D loss: 0.684916, acc: 0.544922]: [A loss: 0.780484, acc: 0.230469]\n",
      "793: [D loss: 0.690423, acc: 0.519531]: [A loss: 0.800061, acc: 0.167969]\n",
      "794: [D loss: 0.686772, acc: 0.554688]: [A loss: 0.781633, acc: 0.199219]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795: [D loss: 0.695413, acc: 0.501953]: [A loss: 0.820037, acc: 0.144531]\n",
      "796: [D loss: 0.689461, acc: 0.552734]: [A loss: 0.755060, acc: 0.292969]\n",
      "797: [D loss: 0.689996, acc: 0.554688]: [A loss: 0.763954, acc: 0.261719]\n",
      "798: [D loss: 0.688095, acc: 0.544922]: [A loss: 0.766513, acc: 0.257812]\n",
      "799: [D loss: 0.693383, acc: 0.539062]: [A loss: 0.774802, acc: 0.222656]\n",
      "800: [D loss: 0.686121, acc: 0.535156]: [A loss: 0.759338, acc: 0.250000]\n",
      "801: [D loss: 0.701716, acc: 0.496094]: [A loss: 0.798659, acc: 0.187500]\n",
      "802: [D loss: 0.686378, acc: 0.541016]: [A loss: 0.773800, acc: 0.218750]\n",
      "803: [D loss: 0.695902, acc: 0.533203]: [A loss: 0.812481, acc: 0.128906]\n",
      "804: [D loss: 0.681186, acc: 0.576172]: [A loss: 0.730629, acc: 0.371094]\n",
      "805: [D loss: 0.697377, acc: 0.515625]: [A loss: 0.849109, acc: 0.082031]\n",
      "806: [D loss: 0.687347, acc: 0.556641]: [A loss: 0.682016, acc: 0.535156]\n",
      "807: [D loss: 0.694005, acc: 0.525391]: [A loss: 0.842105, acc: 0.078125]\n",
      "808: [D loss: 0.697050, acc: 0.498047]: [A loss: 0.711392, acc: 0.441406]\n",
      "809: [D loss: 0.705708, acc: 0.511719]: [A loss: 0.837564, acc: 0.097656]\n",
      "810: [D loss: 0.688716, acc: 0.556641]: [A loss: 0.711354, acc: 0.433594]\n",
      "811: [D loss: 0.688655, acc: 0.535156]: [A loss: 0.801917, acc: 0.199219]\n",
      "812: [D loss: 0.685362, acc: 0.531250]: [A loss: 0.735322, acc: 0.347656]\n",
      "813: [D loss: 0.692636, acc: 0.560547]: [A loss: 0.793897, acc: 0.207031]\n",
      "814: [D loss: 0.679470, acc: 0.566406]: [A loss: 0.784519, acc: 0.230469]\n",
      "815: [D loss: 0.683708, acc: 0.541016]: [A loss: 0.780766, acc: 0.250000]\n",
      "816: [D loss: 0.693051, acc: 0.539062]: [A loss: 0.772860, acc: 0.210938]\n",
      "817: [D loss: 0.684610, acc: 0.562500]: [A loss: 0.770418, acc: 0.226562]\n",
      "818: [D loss: 0.687979, acc: 0.542969]: [A loss: 0.817582, acc: 0.148438]\n",
      "819: [D loss: 0.692103, acc: 0.525391]: [A loss: 0.753829, acc: 0.320312]\n",
      "820: [D loss: 0.696053, acc: 0.523438]: [A loss: 0.797721, acc: 0.218750]\n",
      "821: [D loss: 0.681781, acc: 0.580078]: [A loss: 0.758491, acc: 0.296875]\n",
      "822: [D loss: 0.708326, acc: 0.500000]: [A loss: 0.869594, acc: 0.058594]\n",
      "823: [D loss: 0.692712, acc: 0.529297]: [A loss: 0.720581, acc: 0.386719]\n",
      "824: [D loss: 0.704525, acc: 0.517578]: [A loss: 0.788884, acc: 0.191406]\n",
      "825: [D loss: 0.692384, acc: 0.515625]: [A loss: 0.734548, acc: 0.328125]\n",
      "826: [D loss: 0.693011, acc: 0.544922]: [A loss: 0.774691, acc: 0.226562]\n",
      "827: [D loss: 0.684339, acc: 0.544922]: [A loss: 0.758353, acc: 0.269531]\n",
      "828: [D loss: 0.691226, acc: 0.539062]: [A loss: 0.775545, acc: 0.234375]\n",
      "829: [D loss: 0.680279, acc: 0.572266]: [A loss: 0.757700, acc: 0.292969]\n",
      "830: [D loss: 0.681931, acc: 0.527344]: [A loss: 0.824781, acc: 0.156250]\n",
      "831: [D loss: 0.680970, acc: 0.552734]: [A loss: 0.724651, acc: 0.417969]\n",
      "832: [D loss: 0.716639, acc: 0.531250]: [A loss: 0.886521, acc: 0.042969]\n",
      "833: [D loss: 0.686338, acc: 0.537109]: [A loss: 0.700629, acc: 0.488281]\n",
      "834: [D loss: 0.695179, acc: 0.511719]: [A loss: 0.827675, acc: 0.132812]\n",
      "835: [D loss: 0.694012, acc: 0.505859]: [A loss: 0.754526, acc: 0.281250]\n",
      "836: [D loss: 0.683886, acc: 0.560547]: [A loss: 0.762754, acc: 0.304688]\n",
      "837: [D loss: 0.689151, acc: 0.564453]: [A loss: 0.787546, acc: 0.207031]\n",
      "838: [D loss: 0.689116, acc: 0.556641]: [A loss: 0.777651, acc: 0.207031]\n",
      "839: [D loss: 0.682330, acc: 0.556641]: [A loss: 0.760964, acc: 0.316406]\n",
      "840: [D loss: 0.691642, acc: 0.541016]: [A loss: 0.761378, acc: 0.285156]\n",
      "841: [D loss: 0.695170, acc: 0.521484]: [A loss: 0.788387, acc: 0.199219]\n",
      "842: [D loss: 0.678873, acc: 0.546875]: [A loss: 0.755181, acc: 0.332031]\n",
      "843: [D loss: 0.696061, acc: 0.554688]: [A loss: 0.795979, acc: 0.164062]\n",
      "844: [D loss: 0.694159, acc: 0.539062]: [A loss: 0.767293, acc: 0.281250]\n",
      "845: [D loss: 0.699320, acc: 0.488281]: [A loss: 0.791939, acc: 0.218750]\n",
      "846: [D loss: 0.680715, acc: 0.582031]: [A loss: 0.727743, acc: 0.390625]\n",
      "847: [D loss: 0.693001, acc: 0.519531]: [A loss: 0.815568, acc: 0.152344]\n",
      "848: [D loss: 0.688507, acc: 0.558594]: [A loss: 0.756179, acc: 0.308594]\n",
      "849: [D loss: 0.692082, acc: 0.517578]: [A loss: 0.781993, acc: 0.250000]\n",
      "850: [D loss: 0.698831, acc: 0.496094]: [A loss: 0.767466, acc: 0.265625]\n",
      "851: [D loss: 0.698700, acc: 0.513672]: [A loss: 0.813384, acc: 0.160156]\n",
      "852: [D loss: 0.695282, acc: 0.517578]: [A loss: 0.724210, acc: 0.390625]\n",
      "853: [D loss: 0.694526, acc: 0.541016]: [A loss: 0.827310, acc: 0.121094]\n",
      "854: [D loss: 0.691813, acc: 0.533203]: [A loss: 0.751666, acc: 0.324219]\n",
      "855: [D loss: 0.689393, acc: 0.546875]: [A loss: 0.789583, acc: 0.164062]\n",
      "856: [D loss: 0.675416, acc: 0.589844]: [A loss: 0.746954, acc: 0.335938]\n",
      "857: [D loss: 0.683817, acc: 0.548828]: [A loss: 0.826355, acc: 0.121094]\n",
      "858: [D loss: 0.693204, acc: 0.525391]: [A loss: 0.744043, acc: 0.375000]\n",
      "859: [D loss: 0.689615, acc: 0.542969]: [A loss: 0.801870, acc: 0.156250]\n",
      "860: [D loss: 0.692927, acc: 0.503906]: [A loss: 0.761921, acc: 0.242188]\n",
      "861: [D loss: 0.684535, acc: 0.558594]: [A loss: 0.761943, acc: 0.289062]\n",
      "862: [D loss: 0.682620, acc: 0.576172]: [A loss: 0.782325, acc: 0.226562]\n",
      "863: [D loss: 0.687750, acc: 0.576172]: [A loss: 0.756168, acc: 0.261719]\n",
      "864: [D loss: 0.692147, acc: 0.537109]: [A loss: 0.804787, acc: 0.187500]\n",
      "865: [D loss: 0.683667, acc: 0.576172]: [A loss: 0.747479, acc: 0.320312]\n",
      "866: [D loss: 0.694323, acc: 0.505859]: [A loss: 0.809240, acc: 0.160156]\n",
      "867: [D loss: 0.685111, acc: 0.552734]: [A loss: 0.742020, acc: 0.316406]\n",
      "868: [D loss: 0.700744, acc: 0.509766]: [A loss: 0.801601, acc: 0.171875]\n",
      "869: [D loss: 0.680417, acc: 0.574219]: [A loss: 0.789219, acc: 0.261719]\n",
      "870: [D loss: 0.679009, acc: 0.541016]: [A loss: 0.761171, acc: 0.328125]\n",
      "871: [D loss: 0.701832, acc: 0.507812]: [A loss: 0.909195, acc: 0.039062]\n",
      "872: [D loss: 0.696246, acc: 0.511719]: [A loss: 0.697405, acc: 0.484375]\n",
      "873: [D loss: 0.697025, acc: 0.517578]: [A loss: 0.801123, acc: 0.187500]\n",
      "874: [D loss: 0.695549, acc: 0.509766]: [A loss: 0.763907, acc: 0.242188]\n",
      "875: [D loss: 0.691276, acc: 0.535156]: [A loss: 0.770043, acc: 0.238281]\n",
      "876: [D loss: 0.692858, acc: 0.498047]: [A loss: 0.773466, acc: 0.277344]\n",
      "877: [D loss: 0.697411, acc: 0.515625]: [A loss: 0.774795, acc: 0.230469]\n",
      "878: [D loss: 0.689646, acc: 0.509766]: [A loss: 0.766293, acc: 0.281250]\n",
      "879: [D loss: 0.688756, acc: 0.544922]: [A loss: 0.761725, acc: 0.296875]\n",
      "880: [D loss: 0.693628, acc: 0.519531]: [A loss: 0.815221, acc: 0.136719]\n",
      "881: [D loss: 0.692650, acc: 0.541016]: [A loss: 0.763086, acc: 0.253906]\n",
      "882: [D loss: 0.690451, acc: 0.511719]: [A loss: 0.772962, acc: 0.300781]\n",
      "883: [D loss: 0.683179, acc: 0.550781]: [A loss: 0.782575, acc: 0.214844]\n",
      "884: [D loss: 0.681930, acc: 0.560547]: [A loss: 0.800837, acc: 0.207031]\n",
      "885: [D loss: 0.686387, acc: 0.541016]: [A loss: 0.779006, acc: 0.261719]\n",
      "886: [D loss: 0.690627, acc: 0.539062]: [A loss: 0.794069, acc: 0.226562]\n",
      "887: [D loss: 0.687250, acc: 0.537109]: [A loss: 0.729255, acc: 0.394531]\n",
      "888: [D loss: 0.689996, acc: 0.533203]: [A loss: 0.789279, acc: 0.222656]\n",
      "889: [D loss: 0.689327, acc: 0.533203]: [A loss: 0.784695, acc: 0.226562]\n",
      "890: [D loss: 0.696112, acc: 0.535156]: [A loss: 0.785389, acc: 0.226562]\n",
      "891: [D loss: 0.696333, acc: 0.525391]: [A loss: 0.741730, acc: 0.335938]\n",
      "892: [D loss: 0.702720, acc: 0.500000]: [A loss: 0.836628, acc: 0.082031]\n",
      "893: [D loss: 0.693179, acc: 0.544922]: [A loss: 0.704412, acc: 0.492188]\n",
      "894: [D loss: 0.690829, acc: 0.546875]: [A loss: 0.805340, acc: 0.164062]\n",
      "895: [D loss: 0.691989, acc: 0.517578]: [A loss: 0.739729, acc: 0.320312]\n",
      "896: [D loss: 0.700574, acc: 0.503906]: [A loss: 0.868784, acc: 0.054688]\n",
      "897: [D loss: 0.684865, acc: 0.552734]: [A loss: 0.695978, acc: 0.519531]\n",
      "898: [D loss: 0.699542, acc: 0.527344]: [A loss: 0.803520, acc: 0.195312]\n",
      "899: [D loss: 0.688067, acc: 0.515625]: [A loss: 0.739135, acc: 0.335938]\n",
      "900: [D loss: 0.693343, acc: 0.529297]: [A loss: 0.827626, acc: 0.136719]\n",
      "901: [D loss: 0.689644, acc: 0.531250]: [A loss: 0.726127, acc: 0.351562]\n",
      "902: [D loss: 0.701694, acc: 0.505859]: [A loss: 0.817344, acc: 0.136719]\n",
      "903: [D loss: 0.687342, acc: 0.537109]: [A loss: 0.752215, acc: 0.273438]\n",
      "904: [D loss: 0.695712, acc: 0.507812]: [A loss: 0.790135, acc: 0.187500]\n",
      "905: [D loss: 0.690007, acc: 0.525391]: [A loss: 0.725018, acc: 0.363281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906: [D loss: 0.691355, acc: 0.511719]: [A loss: 0.809046, acc: 0.171875]\n",
      "907: [D loss: 0.685627, acc: 0.560547]: [A loss: 0.694237, acc: 0.519531]\n",
      "908: [D loss: 0.693869, acc: 0.542969]: [A loss: 0.833347, acc: 0.121094]\n",
      "909: [D loss: 0.687435, acc: 0.519531]: [A loss: 0.720845, acc: 0.394531]\n",
      "910: [D loss: 0.688766, acc: 0.550781]: [A loss: 0.783546, acc: 0.246094]\n",
      "911: [D loss: 0.686513, acc: 0.548828]: [A loss: 0.719798, acc: 0.425781]\n",
      "912: [D loss: 0.703427, acc: 0.507812]: [A loss: 0.864602, acc: 0.097656]\n",
      "913: [D loss: 0.688209, acc: 0.541016]: [A loss: 0.718309, acc: 0.425781]\n",
      "914: [D loss: 0.692657, acc: 0.539062]: [A loss: 0.802452, acc: 0.148438]\n",
      "915: [D loss: 0.684961, acc: 0.552734]: [A loss: 0.748261, acc: 0.304688]\n",
      "916: [D loss: 0.698488, acc: 0.519531]: [A loss: 0.794003, acc: 0.199219]\n",
      "917: [D loss: 0.685292, acc: 0.544922]: [A loss: 0.771371, acc: 0.257812]\n",
      "918: [D loss: 0.688744, acc: 0.527344]: [A loss: 0.765190, acc: 0.265625]\n",
      "919: [D loss: 0.689317, acc: 0.541016]: [A loss: 0.812639, acc: 0.156250]\n",
      "920: [D loss: 0.688890, acc: 0.541016]: [A loss: 0.756043, acc: 0.296875]\n",
      "921: [D loss: 0.687334, acc: 0.550781]: [A loss: 0.783170, acc: 0.226562]\n",
      "922: [D loss: 0.696778, acc: 0.511719]: [A loss: 0.747064, acc: 0.312500]\n",
      "923: [D loss: 0.689603, acc: 0.583984]: [A loss: 0.804735, acc: 0.175781]\n",
      "924: [D loss: 0.695658, acc: 0.515625]: [A loss: 0.828988, acc: 0.113281]\n",
      "925: [D loss: 0.691826, acc: 0.519531]: [A loss: 0.737463, acc: 0.375000]\n",
      "926: [D loss: 0.698917, acc: 0.523438]: [A loss: 0.777981, acc: 0.214844]\n",
      "927: [D loss: 0.696412, acc: 0.509766]: [A loss: 0.770036, acc: 0.253906]\n",
      "928: [D loss: 0.681847, acc: 0.580078]: [A loss: 0.753675, acc: 0.308594]\n",
      "929: [D loss: 0.691538, acc: 0.521484]: [A loss: 0.782091, acc: 0.238281]\n",
      "930: [D loss: 0.691945, acc: 0.521484]: [A loss: 0.785120, acc: 0.234375]\n",
      "931: [D loss: 0.692001, acc: 0.535156]: [A loss: 0.814453, acc: 0.164062]\n",
      "932: [D loss: 0.682432, acc: 0.544922]: [A loss: 0.740529, acc: 0.359375]\n",
      "933: [D loss: 0.699195, acc: 0.519531]: [A loss: 0.834294, acc: 0.128906]\n",
      "934: [D loss: 0.681698, acc: 0.576172]: [A loss: 0.688160, acc: 0.484375]\n",
      "935: [D loss: 0.704701, acc: 0.501953]: [A loss: 0.869680, acc: 0.027344]\n",
      "936: [D loss: 0.693442, acc: 0.525391]: [A loss: 0.708025, acc: 0.480469]\n",
      "937: [D loss: 0.683879, acc: 0.550781]: [A loss: 0.786951, acc: 0.218750]\n",
      "938: [D loss: 0.685871, acc: 0.544922]: [A loss: 0.732638, acc: 0.378906]\n",
      "939: [D loss: 0.688213, acc: 0.550781]: [A loss: 0.761965, acc: 0.285156]\n",
      "940: [D loss: 0.694529, acc: 0.503906]: [A loss: 0.761860, acc: 0.316406]\n",
      "941: [D loss: 0.684891, acc: 0.531250]: [A loss: 0.747662, acc: 0.332031]\n",
      "942: [D loss: 0.697474, acc: 0.525391]: [A loss: 0.803854, acc: 0.175781]\n",
      "943: [D loss: 0.693937, acc: 0.525391]: [A loss: 0.756513, acc: 0.300781]\n",
      "944: [D loss: 0.687174, acc: 0.560547]: [A loss: 0.759389, acc: 0.332031]\n",
      "945: [D loss: 0.716383, acc: 0.462891]: [A loss: 0.847337, acc: 0.093750]\n",
      "946: [D loss: 0.693755, acc: 0.503906]: [A loss: 0.712152, acc: 0.398438]\n",
      "947: [D loss: 0.690697, acc: 0.533203]: [A loss: 0.780073, acc: 0.250000]\n",
      "948: [D loss: 0.691244, acc: 0.539062]: [A loss: 0.743133, acc: 0.351562]\n",
      "949: [D loss: 0.694384, acc: 0.513672]: [A loss: 0.775812, acc: 0.242188]\n",
      "950: [D loss: 0.689674, acc: 0.544922]: [A loss: 0.762082, acc: 0.273438]\n",
      "951: [D loss: 0.685368, acc: 0.556641]: [A loss: 0.732531, acc: 0.347656]\n",
      "952: [D loss: 0.694576, acc: 0.546875]: [A loss: 0.800183, acc: 0.140625]\n",
      "953: [D loss: 0.686232, acc: 0.539062]: [A loss: 0.741694, acc: 0.339844]\n",
      "954: [D loss: 0.697316, acc: 0.519531]: [A loss: 0.824417, acc: 0.152344]\n",
      "955: [D loss: 0.681475, acc: 0.570312]: [A loss: 0.702819, acc: 0.472656]\n",
      "956: [D loss: 0.696765, acc: 0.542969]: [A loss: 0.820198, acc: 0.117188]\n",
      "957: [D loss: 0.687626, acc: 0.523438]: [A loss: 0.714295, acc: 0.460938]\n",
      "958: [D loss: 0.693355, acc: 0.519531]: [A loss: 0.788587, acc: 0.179688]\n",
      "959: [D loss: 0.688299, acc: 0.548828]: [A loss: 0.702022, acc: 0.492188]\n",
      "960: [D loss: 0.695549, acc: 0.527344]: [A loss: 0.793632, acc: 0.167969]\n",
      "961: [D loss: 0.689581, acc: 0.519531]: [A loss: 0.737611, acc: 0.339844]\n",
      "962: [D loss: 0.689265, acc: 0.539062]: [A loss: 0.782552, acc: 0.195312]\n",
      "963: [D loss: 0.689933, acc: 0.544922]: [A loss: 0.725757, acc: 0.386719]\n",
      "964: [D loss: 0.693761, acc: 0.535156]: [A loss: 0.808548, acc: 0.164062]\n",
      "965: [D loss: 0.699335, acc: 0.496094]: [A loss: 0.764569, acc: 0.234375]\n",
      "966: [D loss: 0.692545, acc: 0.544922]: [A loss: 0.760509, acc: 0.281250]\n",
      "967: [D loss: 0.697654, acc: 0.507812]: [A loss: 0.774233, acc: 0.222656]\n",
      "968: [D loss: 0.690454, acc: 0.513672]: [A loss: 0.754062, acc: 0.285156]\n",
      "969: [D loss: 0.691866, acc: 0.537109]: [A loss: 0.719117, acc: 0.433594]\n",
      "970: [D loss: 0.696292, acc: 0.527344]: [A loss: 0.786879, acc: 0.195312]\n",
      "971: [D loss: 0.704434, acc: 0.476562]: [A loss: 0.788811, acc: 0.183594]\n",
      "972: [D loss: 0.694033, acc: 0.498047]: [A loss: 0.761281, acc: 0.253906]\n",
      "973: [D loss: 0.687699, acc: 0.531250]: [A loss: 0.746012, acc: 0.320312]\n",
      "974: [D loss: 0.686857, acc: 0.566406]: [A loss: 0.764463, acc: 0.269531]\n",
      "975: [D loss: 0.680910, acc: 0.535156]: [A loss: 0.745355, acc: 0.339844]\n",
      "976: [D loss: 0.692753, acc: 0.541016]: [A loss: 0.807264, acc: 0.160156]\n",
      "977: [D loss: 0.682480, acc: 0.566406]: [A loss: 0.716396, acc: 0.460938]\n",
      "978: [D loss: 0.688540, acc: 0.523438]: [A loss: 0.821011, acc: 0.160156]\n",
      "979: [D loss: 0.682863, acc: 0.560547]: [A loss: 0.710799, acc: 0.460938]\n",
      "980: [D loss: 0.695514, acc: 0.527344]: [A loss: 0.806979, acc: 0.160156]\n",
      "981: [D loss: 0.689141, acc: 0.527344]: [A loss: 0.730525, acc: 0.371094]\n",
      "982: [D loss: 0.686475, acc: 0.548828]: [A loss: 0.763477, acc: 0.292969]\n",
      "983: [D loss: 0.688535, acc: 0.548828]: [A loss: 0.757408, acc: 0.285156]\n",
      "984: [D loss: 0.684979, acc: 0.546875]: [A loss: 0.801927, acc: 0.207031]\n",
      "985: [D loss: 0.685028, acc: 0.537109]: [A loss: 0.760123, acc: 0.332031]\n",
      "986: [D loss: 0.688970, acc: 0.560547]: [A loss: 0.770333, acc: 0.292969]\n",
      "987: [D loss: 0.694792, acc: 0.539062]: [A loss: 0.798716, acc: 0.183594]\n",
      "988: [D loss: 0.688339, acc: 0.529297]: [A loss: 0.757083, acc: 0.312500]\n",
      "989: [D loss: 0.688480, acc: 0.535156]: [A loss: 0.785978, acc: 0.234375]\n",
      "990: [D loss: 0.703275, acc: 0.507812]: [A loss: 0.765589, acc: 0.250000]\n",
      "991: [D loss: 0.686381, acc: 0.552734]: [A loss: 0.743439, acc: 0.328125]\n",
      "992: [D loss: 0.699216, acc: 0.515625]: [A loss: 0.790285, acc: 0.257812]\n",
      "993: [D loss: 0.689551, acc: 0.531250]: [A loss: 0.732933, acc: 0.359375]\n",
      "994: [D loss: 0.687406, acc: 0.564453]: [A loss: 0.760117, acc: 0.347656]\n",
      "995: [D loss: 0.695767, acc: 0.523438]: [A loss: 0.833593, acc: 0.117188]\n",
      "996: [D loss: 0.691685, acc: 0.541016]: [A loss: 0.785564, acc: 0.246094]\n",
      "997: [D loss: 0.690216, acc: 0.519531]: [A loss: 0.739477, acc: 0.347656]\n",
      "998: [D loss: 0.698653, acc: 0.494141]: [A loss: 0.763697, acc: 0.277344]\n",
      "999: [D loss: 0.683351, acc: 0.558594]: [A loss: 0.765050, acc: 0.273438]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    mnist_dcgan = MNIST_DCGAN()\n",
    "    mnist_dcgan.train(train_steps=1000, batch_size=256, save_interval=500)\n",
    "    mnist_dcgan.plot_images(fake=True)\n",
    "    mnist_dcgan.plot_images(fake=False, save2file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
